{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0814ce26-0d88-44a2-b11f-861e9ef210b2",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability Framework\n",
    "\n",
    "The mechanistic interpretability is a specific framework for looking at the mechanisms in transformer models in terms of operations on the residual stream. The main intuition is to break down the high dimensional models into easily understandable composition of mechanisms/components. \n",
    "\n",
    "<img src=\"../images/mechtrans.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075ea47-0d39-4ff4-b60e-e3642bc6b068",
   "metadata": {},
   "source": [
    "## Important Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67175488-ebf7-447c-9dcc-b41f8e05b143",
   "metadata": {},
   "source": [
    "### Residual Stream\n",
    "\n",
    "The initial input token encodings which parallely undergo transformations throughout a transformer. All components of a transformer (the token embedding, attention heads, MLP layers, and unembedding) communicate with each other by reading and writing to different subspaces of the residual stream. Rather than analyze the residual stream vectors, it can be helpful to decompose the residual stream into all these different communication channels, corresponding to paths through the model.\n",
    "\n",
    "Features of the residual stream: \n",
    "\n",
    "1. **Linear Structuring**: Any communication to and from the residual stream only happens in terms of linear operations- addition or linear map- thus endowing transformers a great deal of linearity. This also has the consequence that residual stream doesn't have a privileged basis.\n",
    "2. **Selective Flow**: The information flow via the residual stream is selective as the model can \"select\" which layers of the transformers it routes a token through where the selectivity is practically implemented as model weights. \n",
    "\n",
    "\n",
    "Note: Privileged basis (sometimes called a \"preferred basis\") for a set of vectors refers to a particular choice of basis vectors that simplifies calculations, enhances understanding, or aligns with specific properties of the vector space such as the $n$ coordinate vectors in a $\\mathbb{R}^n $ space. In the case of transformers, privileged basis for a set of vectors would be those that enhance interpretability or make calculations easier. Specifically for mechanistic interpretability, the task then is to decompose a model in terms of the components that do have privileged basis (embedding, attention, MLP) where privilege is a spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c1347-55c5-4886-9703-de1fc28f1959",
   "metadata": {},
   "source": [
    "### Virtual Weights\n",
    "\n",
    "The linearity of the residual stream means that the amount of connection between any two layer can be quantified as \"virtual weights\" that indicate extent to which the later layer reads the information written by the previous layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af5ebe-8de9-4fad-9840-6f0cd5a9f033",
   "metadata": {},
   "source": [
    "### Superposition\n",
    "\n",
    "Due to the dimensionality difference in the residual stream and other model components leading to bottleneck activations, superposition occurs where each dimension is not a unique interpretable feature (since important features like \"London\" are sparse) and it instead encodes a mix of features.  The model thus finds a balance between trying to encode most features and being able to read them out easily. \n",
    "\n",
    "The high load on residual stream bandwidth that leads to superposition also leads to the memory roles of attention & MLP where they read in information and write out the negative version from the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca4dcb-52f7-4841-9818-1f861f715bdf",
   "metadata": {},
   "source": [
    "### Attention Circuits\n",
    "\n",
    "The attention mechanism in transformers can be considered to have the following important features: \n",
    "\n",
    "1. There are two main circuits- QK(which computes relations between tokens) and OV (which computes how each token affects the output if attended to).\n",
    "\n",
    "2. The attention heads are independent and additive.\n",
    "\n",
    "<img src=\"../images/atthead.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)   \n",
    "\n",
    "3. The attention heads move information i.e they read information from one token and write it to the residual stream of another token. Within an attention block, the series of multiplications are actually associative and the order doesn't really matter. For example, the $W_{OV}$ can be factorized in any way to get a $W_{O}$ and a $W_{V}$, same goes for $W_{QK}$ though OV and QK are very different functions.\n",
    "4. The composition of attention heads forms induction heads which greatly increase expressivity of transformers. Key and query composition are very different from value composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05770068-ba24-4b20-ab3c-bca9338930c6",
   "metadata": {},
   "source": [
    "## Reverse Engineering\n",
    "\n",
    "Using toy attention-only  models, we can analyse characteristic behaviours of transformers: \n",
    "\n",
    "1. **Zero layer Transformers**: They emulate bigram statistics.\n",
    "2. **One layer  Transformers**: They emulate bigram + skipgram statistics. Trigrams are hard to learn because positional encodings only encode before and after and not really individual positional information. \n",
    "3. **Two layer Transformers**: At this stage, the composition of attention Heads across layers leads to formation of *induction heads* which are equivalent to a simple in-context learning algorithms. The formation of these induction heads lead to a turning point for emergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934784b-52a5-43ee-bc05-0753b937010d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
