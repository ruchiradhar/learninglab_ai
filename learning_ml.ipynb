{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955b0ffe-c5d7-47d2-9320-08f7cf065d34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "The main premise of discussion here is \"machine learning\" but what do we mean when we say learning? It refers to the process where models predict something, calculate how wrong they were and learn from their mistake by updating choices that led to the initial prediction. \n",
    "\n",
    "The typical machine learning loop looks like the following: \n",
    "\n",
    "1. **Forward Pass**: Given input x and parameter w, model predicts the output.\n",
    "2. **Loss Calculation**: The model calculates the loss i.e the difference between actual output and predicted output.\n",
    "3. **Local Gradient Computation**: The model calculates the derivatives of output wrt input for every computation point in the model.\n",
    "4. **Backward Pass**: The model computes the blame of each weight for loss i.e it calculates the derivative of the loss function wrt the weights by using the local gradients via chain rule.\n",
    "5. **Parameter Update**: The model weights are updated to reduce loss using algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a88311-425e-42f6-934d-14bf1c76531c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example Setup\n",
    "\n",
    "- **Input**: $ x = 2 $\n",
    "- **Target Output/Label**: $10$\n",
    "- **Model Function**: $y= w \\times x$\n",
    "- **Initial Weight**:$ w = 3 $\n",
    "\n",
    "\n",
    "**Step 1: Forward Pass**\n",
    "\n",
    "$$\\text{prediction} = w \\times x = 3 \\times 2 = 6$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Loss Calculation**\n",
    "\n",
    "Using the Mean Squared Error (MSE) loss function:\n",
    "\n",
    "$$\\text{loss} = (y - \\text{prediction})^2 = (10 - 6)^2 = 16$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Local Gradient Computation**\n",
    "\n",
    "\n",
    "- **Derivative of Loss with respect to Prediction**:\n",
    "\n",
    "   $$\\frac{\\partial \\text{loss}}{\\partial \\text{prediction}} = 2 \\times (y - \\text{prediction}) = 2 \\times (10 - 6) = 2 \\times 4 = 8$$\n",
    "\n",
    "- **Derivative of Prediction with respect to Weight**:\n",
    "\n",
    "   $$\\frac{\\partial \\text{prediction}}{\\partial w} = x = 2$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Backward Pass**\n",
    "\n",
    "Using the chain rule, we get\n",
    "\n",
    "   $$\\frac{\\partial \\text{loss}}{\\partial w} = \\frac{\\partial \\text{loss}}{\\partial \\text{prediction}} \\times \\frac{\\partial \\text{prediction}}{\\partial w} = 8 \\times 2 = 16$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 5: Parameter Update**\n",
    "\n",
    "Using gradient descent with a learning rate $\\alpha = 0.1 $:\n",
    "\n",
    "$$w = w - \\alpha \\times \\frac{\\partial \\text{loss}}{\\partial w}$$\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "$$w = 3 - 0.1 \\times 16 = 3 - 1.6 = 1.4$$\n",
    "\n",
    "After this update, the new weight $ w $ is 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c3aa-f578-4ddc-a989-d694539f72c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pytorch \n",
    "\n",
    "PyTorch is an important tensor library used for machine learning which supports differentiable vectors. So, let's take a look at some basic operations in pytorch tensors first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b79e33c-3eff-4ce9-90e3-42205d92fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6981332-4eb1-4e65-a63a-fdea66a9e3d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a1bd3d-6f46-4273-993f-d448b264d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]]) torch.float32 torch.Size([2, 2])\n",
      "tensor([[3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[4, 3],\n",
      "        [8, 5]])\n",
      "tensor([[1., 3.],\n",
      "        [2., 5.]])\n",
      "tensor([[2, 3]])\n",
      "5\n",
      "tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n",
      "tensor([[2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# creating special tensors with given dimensions\n",
    "x= torch.empty(2,2) # also torch.rand(), torch.zeros(), torch.ones()\n",
    "print(x, x.dtype, x.size())\n",
    "\n",
    "# elementwise addition/subtraction\n",
    "# in torch all func_ formats will do inplace operations\n",
    "x=torch.tensor ([[2,3],[4,5]])\n",
    "y=torch.tensor ([[1,1],[1,1]])\n",
    "print(x+y) # also with x.add_(y) and torch.add(x,y)\n",
    "\n",
    "# elementwise multiplication/division\n",
    "x=torch.tensor ([[2,3],[4,5]])\n",
    "y=torch.tensor ([[2,1],[2,1]])\n",
    "print(torch.mul(x,y))\n",
    "print(torch.div(x,y))\n",
    "\n",
    "# slicing\n",
    "x=torch.tensor ([[2,3],[4,5]])\n",
    "print(x[:1,:])\n",
    "print(x[1,1].item()) # .item() can be used when tensor's single element is to be extracted\n",
    "\n",
    "# reshaping\n",
    "x=torch.tensor ([[2,3],[4,5]])\n",
    "y=x.view(4,1) #giving new dimensions\n",
    "print(y)\n",
    "z= x.view (-1,4)  # specifying one dim for reshape and -1 indicates no preference where torch will assume dim\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9272273-13d4-4aa5-abb1-ca4428558280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]]) [[2. 2.]\n",
      " [2. 2.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]] tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# tensor <> numpy conversion\n",
    "# if on cpu, then they both occupy same memory (can check with inplace addition on 1 and other is affected)\n",
    "# check gpu with torch.backends.mps.is_available() followed by device=torch.device and .to(device)\n",
    "a=torch.ones(2,2)\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)\n",
    "a.add_(1) # inplace addition in torch\n",
    "print(a,b)\n",
    "\n",
    "c=np.ones([3,3])\n",
    "print(c)\n",
    "d=torch.from_numpy(c)\n",
    "print(d)\n",
    "c+=(1) # inplace addition in numpy\n",
    "print(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950a949-d3ef-4da4-ac49-47d3f287d2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed546398-256b-4cf3-a5b6-e5fd53e9bd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bd9be-0c1d-41eb-9a4f-4886a611afb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e614ea8-90ab-41ae-a32e-93676060653e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gradient Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4aba5ba-b491-4b47-a963-c8613c60dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor(18., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# differentiability in torch\n",
    "# when we want to do backprop and calculate gradient on a tensor we need to specify differentiability\n",
    "# set with requires_grad=True\n",
    "x=torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# define series of functions where x is a variable\n",
    "# torch creates a computational graph with grad_fn \n",
    "y=x+2\n",
    "z=y*y*2\n",
    "z=z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5294a71d-730b-499c-8b37-41cdaf5a93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000],\n",
      "        [0.1100, 0.1100]])\n",
      "tensor([[3.1000, 3.1000],\n",
      "        [3.1100, 3.1100]])\n"
     ]
    }
   ],
   "source": [
    "# calculate gradient of y wrt x i.e dy/dx via jacobian vector and chain rule\n",
    "# if y not scalar, needs a vector of same size as y passed in \n",
    "v=torch.tensor([[0.1,0.1],[0.11,0.11]])\n",
    "y.backward(v) \n",
    "print(x.grad) # the gradients are stored for x\n",
    "\n",
    "# calculate gradient of z wrt x i.e dz/dx via jacobian vector and chain rule\n",
    "z.backward() # if z not scalar, needs a vector of same size passed in \n",
    "print(x.grad) # the gradients are stored for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dacf015-92de-410f-9bc3-65daf6556c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1691, -0.5016, -0.1829], requires_grad=True)\n",
      "tensor([ 0.1691, -0.5016, -0.1829])\n",
      "tensor([ 0.1691, -0.5016, -0.1829])\n",
      "tensor([2.1691, 1.4984, 1.8171])\n"
     ]
    }
   ],
   "source": [
    "# preventing torch from tracking history and grad_fn attribute\n",
    "# in training weight updation, gradient should not be part of the loop for multiple updation\n",
    "x=torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# Option 1 \n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "#Option 2\n",
    "y=x.detach()\n",
    "print(y)\n",
    "\n",
    "# Option 3\n",
    "with torch.no_grad(): \n",
    "    y=x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b503fff-5cc7-4a37-bc90-82e9cd9a6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----defining inputs------\n",
      "tensor([[-0.0551, -0.2824],\n",
      "        [ 1.2463,  0.6389]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "-----training loop starts------\n",
      "tensor([[-0.0337,  0.1885],\n",
      "        [-0.0371,  0.2074]])\n",
      "tensor([[-0.0337,  0.1885],\n",
      "        [-0.0371,  0.2074]])\n"
     ]
    }
   ],
   "source": [
    "# toy training loop (from scratch)\n",
    "# let toy function be y=wx where x is input and w is weight matrix\n",
    "print(\"-----defining inputs------\")\n",
    "x=torch.randn(2,2)\n",
    "print(x)\n",
    "w=torch.ones([2,2],requires_grad=True) # weights need gradients to be updated\n",
    "print(w)\n",
    "\n",
    "print(\"-----training loop starts------\")\n",
    "\n",
    "# write training loop for updation\n",
    "for epoch in range(2): \n",
    "    # define output function within loop\n",
    "    y=torch.matmul(w,x)\n",
    "    v=torch.tensor([[0.1,0.1],[0.11,0.11]])\n",
    "    y.backward(v) # irl we do loss.backward() which calculates local gradients for all weight \n",
    "    print(w.grad) # now each parameter has a gradient associated which is multiplied to get loss gradient as a whole\n",
    "    w.grad.zero_() # ensure gradients are not being added up for every pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f59fe-92a2-4af8-8d2c-30ce42847f21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4bdb71-bcd6-433a-9af2-9a1940b1dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(-2.)\n",
      "tensor(1.2000, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# defining inputs\n",
    "x=torch.tensor(1.)\n",
    "y=torch.tensor(2.0)\n",
    "w= torch.tensor(1.0,requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "# forward pass\n",
    "y_hat= w*x\n",
    "\n",
    "# loss calculation\n",
    "loss=(y_hat-y)**2\n",
    "\n",
    "# backward pass\n",
    "# includes local grdaient computation\n",
    "loss.backward()\n",
    "print(w.grad) # the gradient of weight wrt loss\n",
    "\n",
    "# parameter update\n",
    "# use gradient descent\n",
    "alpha=0.1 #set learning rate\n",
    "w=w-alpha*w.grad\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20c1a4-e4b0-445d-9765-adc36f65942b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "The typical steps for model specification include: \n",
    "\n",
    "1. Design model (input, output, forward pass)\n",
    "2. Construct loss\n",
    "3. Construct optimizer\n",
    "\n",
    "The typical steps for training loop include: \n",
    "\n",
    "1. Forward Pass: Get predicted output\n",
    "2. Loss: Get loss\n",
    "3. Backward: Do backward on calculated loss\n",
    "4. Optimization: Update parameters\n",
    "5. Zeroing Gradients: Set gradients to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8a9c1-bead-487d-98d7-e4ef44282791",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Toy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c301a2c7-93a5-4259-ba1a-7a4de606ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training:f(5)=0.000\n",
      "epoch 1:w = 1.200, loss=30.00000000\n",
      "epoch 2:w = 1.680, loss=4.80000067\n",
      "epoch 3:w = 1.872, loss=0.76800019\n",
      "epoch 4:w = 1.949, loss=0.12288000\n",
      "epoch 5:w = 1.980, loss=0.01966083\n",
      "epoch 6:w = 1.992, loss=0.00314574\n",
      "epoch 7:w = 1.997, loss=0.00050332\n",
      "epoch 8:w = 1.999, loss=0.00008053\n",
      "epoch 9:w = 1.999, loss=0.00001288\n",
      "epoch 10:w = 2.000, loss=0.00000206\n",
      "Prediction after training:f(5)=9.999\n"
     ]
    }
   ],
   "source": [
    "# implementation from scratch using numpy & manual gradient\n",
    "# regression function general form: y=w*x +b \n",
    "# for this example, ideal w=2 and ignore b: y=2*x\n",
    "\n",
    "# inputs\n",
    "X=np.array([1,2,3,4],dtype=np.float32)\n",
    "# outputs\n",
    "Y= np.array([2,4,6,8], dtype=np.float32)\n",
    "# initialize weights\n",
    "w=0.0 \n",
    "\n",
    "# model training\n",
    "# step 1: forward pass\n",
    "def forward(x): \n",
    "    return w*x\n",
    "\n",
    "# step 2: loss calculation with MSE\n",
    "def loss(y,y_predicted): \n",
    "    return ((y_predicted-y)**2).mean() \n",
    "\n",
    "# step 3: backward pass\n",
    "# MSE=1/N*(w*x-y)**2\n",
    "# dLoss/dw=1/N 2x(w*x-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "\n",
    "# implementing training\n",
    "# initial prediction\n",
    "print(f'Prediction before training:f(5)={forward(5):.3f}')\n",
    "# setting hyperparameters\n",
    "learning_rate=0.01\n",
    "n_iters=10\n",
    "# training loop\n",
    "for epoch in range(n_iters): \n",
    "    # forward\n",
    "    y_pred=forward(X)\n",
    "    # loss\n",
    "    l=loss(Y,y_pred)\n",
    "    # backward \n",
    "    dlw=gradient (X,Y,y_pred)\n",
    "    # parameter updation\n",
    "    w=w-learning_rate*dlw\n",
    "\n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch+1}:w = {w:.3f}, loss={l:.8f}')\n",
    "# final prediction\n",
    "print(f'Prediction after training:f(5)={forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb9314f-727a-47ab-87a4-3ce1bce1b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training:f(5)=0.000\n",
      "epoch 1:w = 0.300, loss=30.00000000\n",
      "epoch 2:w = 0.555, loss=21.67499924\n",
      "epoch 3:w = 0.772, loss=15.66018772\n",
      "epoch 4:w = 0.956, loss=11.31448650\n",
      "epoch 5:w = 1.113, loss=8.17471695\n",
      "epoch 6:w = 1.246, loss=5.90623236\n",
      "epoch 7:w = 1.359, loss=4.26725292\n",
      "epoch 8:w = 1.455, loss=3.08308983\n",
      "epoch 9:w = 1.537, loss=2.22753215\n",
      "epoch 10:w = 1.606, loss=1.60939169\n",
      "Prediction after training:f(5)=8.031\n"
     ]
    }
   ],
   "source": [
    "# implementation from scratch using pytorch (no manual gradient needed)\n",
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "w= torch.tensor(0.0,dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# forward\n",
    "def forward(x): \n",
    "    return w*x\n",
    "#loss\n",
    "def loss(y,y_pred): \n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# optimizer\n",
    "lr=0.01\n",
    "\n",
    "# initial prediction\n",
    "print(f'Prediction before training:f(5)={forward(5):.3f}')\n",
    "\n",
    "# training loop\n",
    "n_iters=10\n",
    "for epoch in range(n_iters): \n",
    "    y_pred=forward(X)\n",
    "    l= loss(Y,y_pred)\n",
    "    l.backward()\n",
    "    with torch.no_grad():\n",
    "        w-=lr*w.grad # inplace assignment to avoid breaking of w's computation graph wrt l\n",
    "    w.grad.zero_()\n",
    "    print(f'epoch {epoch+1}:w = {w:.3f}, loss={l:.8f}')\n",
    "\n",
    "# final prediction\n",
    "print(f'Prediction after training:f(5)={forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94c2b01-d733-41a1-a0ab-0f8f6c7b6e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training:f(5)=-4.286\n",
      "epoch 1:w = -0.293, loss=64.87908936\n",
      "epoch 2:w = 0.075, loss=45.02212524\n",
      "epoch 3:w = 0.381, loss=31.24377632\n",
      "epoch 4:w = 0.636, loss=21.68325424\n",
      "epoch 5:w = 0.848, loss=15.04938126\n",
      "epoch 6:w = 1.025, loss=10.44625664\n",
      "epoch 7:w = 1.173, loss=7.25222349\n",
      "epoch 8:w = 1.296, loss=5.03592730\n",
      "epoch 9:w = 1.398, loss=3.49806452\n",
      "epoch 10:w = 1.483, loss=2.43095160\n",
      "Prediction after training:f(5)=7.544\n"
     ]
    }
   ],
   "source": [
    "# pipeline implementation\n",
    "X=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "n_samples,n_features=X.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "# forward\n",
    "input_size=n_features\n",
    "output_size= n_features\n",
    "model=nn.Linear(input_size,output_size)\n",
    "    \n",
    "# loss\n",
    "loss=nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "lr=0.01\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "# initial prediction\n",
    "# test tensor\n",
    "X_test=torch.tensor([5], dtype=torch.float32)\n",
    "print(f'Prediction before training:f(5)={model(X_test).item():.3f}')\n",
    "                          \n",
    "# training loop  \n",
    "n_iters=10\n",
    "for epoch in range(n_iters): \n",
    "    y_pred=model(X)\n",
    "    l= loss(Y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    [w,b]=model.parameters()\n",
    "    print(f'epoch {epoch+1}:w = {w[0][0].item():.3f}, loss={l:.8f}')\n",
    "\n",
    "# final prediction\n",
    "print(f'Prediction after training:f(5)={model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d447e7-764b-4600-a330-536a32bf8ebd",
   "metadata": {},
   "source": [
    "## Real Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60bbbce5-a093-46c5-af00-5cdb06dbb6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 4007.7766\n",
      "epoch: 20, loss = 2825.2451\n",
      "epoch: 30, loss = 2019.2202\n",
      "epoch: 40, loss = 1469.7080\n",
      "epoch: 50, loss = 1094.9955\n",
      "epoch: 60, loss = 839.4261\n",
      "epoch: 70, loss = 665.0818\n",
      "epoch: 80, loss = 546.1240\n",
      "epoch: 90, loss = 464.9413\n",
      "epoch: 100, loss = 409.5280\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "# cast to float Tensor\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_predicted = model(X)\n",
    "    #loss\n",
    "    loss = criterion(y_predicted, y)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimization\n",
    "    optimizer.step()\n",
    "    # zero grad \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d2a69-b3f8-4a4f-81fd-fb552bfdcd42",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99196cf3-f00e-4bcb-b086-9028108f3611",
   "metadata": {},
   "source": [
    "## Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68996291-83cc-4200-ad6d-9916e03cb625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.5664\n",
      "epoch: 20, loss = 0.4620\n",
      "epoch: 30, loss = 0.3975\n",
      "epoch: 40, loss = 0.3535\n",
      "epoch: 50, loss = 0.3214\n",
      "epoch: 60, loss = 0.2967\n",
      "epoch: 70, loss = 0.2770\n",
      "epoch: 80, loss = 0.2609\n",
      "epoch: 90, loss = 0.2474\n",
      "epoch: 100, loss = 0.2358\n",
      "accuracy: 0.9123\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "# load and split\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "# reshape output \n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "# model\n",
    "# linear model f = wx + b with sigmoid at the end\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Model(n_features)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() #implement by hand if possible\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward\n",
    "    y_pred = model(X_train)\n",
    "    # loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # optimizer\n",
    "    optimizer.step()\n",
    "    # zero grad \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# evaluation \n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round() # get softmax in real\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "268d5371-e62e-45ab-b7d3-4d46949607a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy softmaxed output is:[0.65900114 0.24243297 0.09856589]\n",
      "The torch softmaxed output is:tensor([0.6590, 0.2424, 0.0986])\n",
      "The numpy cross entropy loss is -0.35667494393873245\n",
      "The torch cross entropy loss is 0.4170299470424652\n",
      "The torch cross entropy loss prediction is tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# For logistic regression,  Softmax and CE Loss are important for getting results\n",
    "\n",
    "# SOFTMAX: converts scores to probabilities\n",
    "# in numpy\n",
    "def softmax(x): \n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "x=np.array ([2.0,1.0,0.1])\n",
    "outputs=softmax(x)\n",
    "print(f'The numpy softmaxed output is:{outputs}')\n",
    "\n",
    "# in pytorch\n",
    "x=torch.tensor ([2.0,1.0,0.1])\n",
    "outputs=torch.softmax(x,dim=0)\n",
    "print(f'The torch softmaxed output is:{outputs}')\n",
    "\n",
    "\n",
    "# CROSS ENTROPY LOSS: measures divergence of two distributions\n",
    "# Y should be one hot encoding\n",
    "# Y_pred should be probabilities\n",
    "# in numpy\n",
    "def celoss(actual, predicted): \n",
    "    loss= np.sum(actual*np.log(predicted))\n",
    "    return loss\n",
    "Y= np.array ([1,0,0])\n",
    "Y_pred=np.array([0.7,0.2,0.1])\n",
    "l=celoss(Y, Y_pred)\n",
    "print(f'The numpy cross entropy loss is {l}')\n",
    "# in torch\n",
    "loss=nn.CrossEntropyLoss() #already applies nn.LogSoftmax + nn.NLLLoss\n",
    "# here Y needs to be class labels (not one hot) \n",
    "Y= torch.tensor ([0])\n",
    "Y_pred=torch.tensor([[2.0,1.0,0.1]]) #nsamples X nclasses\n",
    "# here Y_pred needs to be raw logit scores (not softmax)\n",
    "l= loss(Y_pred,Y)\n",
    "_,predictions=torch.max(Y_pred,1)\n",
    "print(f'The torch cross entropy loss is {l.item()}')\n",
    "print(f'The torch cross entropy loss prediction is {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c15594-08b9-4126-9cc1-a6981f1ba790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8791a-a646-4954-be2e-218bb0208f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ad719-5a85-43cb-ba51-4a25e70be390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869d9f35-181f-4955-9cf0-0612b88a9de2",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Basic assumption: Features must be independent for naive bayes to apply. \n",
    "\n",
    "Advantages: It works well with a large number of features and sparse matrices.\n",
    "\n",
    "Disadvantages: Inadvertent correlation of features will lower performance. \n",
    "\n",
    "Feature Scaling: Not required\n",
    "\n",
    "Impact of missing values: Not impacted\n",
    "\n",
    "Impact of outliers: Not impacted\n",
    "\n",
    "Applications: NLP, Sentiment Analysis, Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71d262-5a83-483a-a8c9-7d6fb1094447",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea3345-2b7b-4faf-8156-2cbc41a76ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
