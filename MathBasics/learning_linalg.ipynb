{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0a11b4-f38c-4110-b5ff-2f4ab7b244a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90ac52-b03d-40a7-8f0b-a22269e3417e",
   "metadata": {},
   "source": [
    "## Linear Algebra Basics\n",
    "\n",
    "It refers to solving for unknowns within a system of linear equations where we can have many equations (multiple data points) and many unknowns in a equation (multiple parameters). \n",
    "\n",
    "Current Uses: \n",
    "\n",
    "- Solving for unknowns in ML/DL algorithms\n",
    "- Reducing dimensionality of data while preserving information (PCA)\n",
    "- Eigenvector scoring of webpages\n",
    "- Recommender systems (SVD) \n",
    "- NLP like topic modelling or semantic analysis (SVD, Matrix Factorization)\n",
    "\n",
    "\n",
    "There can be different number of solutions: \n",
    "\n",
    "- One solution (intersecting graphs)\n",
    "- No solution (parallel graphs)\n",
    "- Infinite solutions (overlapping graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26529d0-b830-431b-809a-cc47160a045e",
   "metadata": {},
   "source": [
    "## Common Data Structures \n",
    "\n",
    "The most important structure for linear algbera is Tensors, which are arrays of numbers. They are the ML generalization of vectors/matrices to any number of dimensions. \n",
    "\n",
    "- 0 dim tensor: Scalar which has a magnitude only\n",
    "- 1 dim tensor: Vector which is an array or a list of numbers\n",
    "- 2 dim tensor: Matrix which is a flat table of numbers\n",
    "- 3 dim tensor: Tensor which is a 3D table of numbers\n",
    "\n",
    "Libraries: Pytorch and tensorflow are the most popular automatic differentiation libraries where Pytorch is more popular due to its pythonic tensors which behave like NumPy arrays but are better suited for parallel computation in GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f83e5-233b-417b-804d-e69d7b473587",
   "metadata": {},
   "source": [
    "### Scalars\n",
    "\n",
    "It is a single number with no dimensions which is denoted in lowercase like $x$. Scalars are typically typed.\n",
    "\n",
    "For pytorch, it is easy to create tensors while for tensorflow we need to use a wrapper like `tf.Variable` or `tf.constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4c0304f-ec3a-4c16-986f-66658feda6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., dtype=torch.float16)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n",
      "tensor(20., dtype=torch.float16)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n",
      "tensor(45., dtype=torch.float16)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# scalars in pytorch\n",
    "x_pt=torch.tensor(25,dtype=torch.float16)\n",
    "print(x_pt)\n",
    "print(type(x_pt))\n",
    "print(x_pt.shape) #no dimensionality\n",
    "\n",
    "y_pt=torch.tensor(20,dtype=torch.float16)\n",
    "print(y_pt)\n",
    "print(type(y_pt))\n",
    "print(y_pt.shape) #no dimensionality\n",
    "\n",
    "# adding tensors\n",
    "z_pt= x_pt+ y_pt\n",
    "print(z_pt)\n",
    "print(type(z_pt))\n",
    "print(z_pt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f1d86-51ed-4409-8524-a5b165d1ae37",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "It is an one dimensional array of numbers arranged in order which can be considered to represent a point in n-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81c2c184-89fe-4a56-9a95-94d579cb4396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25  2  5]\n",
      "3\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "[[25  2  5]]\n",
      "1\n",
      "(1, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Vectors in Numpy \n",
    "# one dim vector\n",
    "x= np.array([25,2,5])\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "\n",
    "# matrix style vector\n",
    "# each inner bracket is a row and number of elements within the inner bracket is number of columns\n",
    "y=np.array([[25,2,5]])\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(y.shape)\n",
    "print(type(y))\n",
    "\n",
    "# zero vector\n",
    "z=np.zeros(3)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386caa1-9eb3-4e52-a466-ca4e5d5fb11b",
   "metadata": {},
   "source": [
    "#### Vector Transpose\n",
    "\n",
    "It consists of reversing the row and column identities for each element in a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7edc0aa-1e75-4c97-9eea-153efc5e7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Transpose\n",
    "x_t= x.T\n",
    "print(x_t)\n",
    "print(x_t.shape)\n",
    "\n",
    "\n",
    "y_t= y.T\n",
    "print(y_t)\n",
    "print(y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244dbbc-635a-483c-af1e-e7f07e0ad55f",
   "metadata": {},
   "source": [
    "#### Vector Normalization \n",
    "\n",
    "It refers to dividing the elements of a vector by its norm which represents length of the vector from the origin. \n",
    "\n",
    "- Distance Calculation: The norm can also be used to express distances between two vectors.\n",
    "- Unit Vectorization: The norm can be used to create a unit vector after normalization when length is 1. \n",
    "\n",
    "\n",
    "The general Lp Norm Formula is\n",
    "\n",
    "$$ ||x||_p = (\\sum|x_i|^p)^{1/p}  $$\n",
    "\n",
    "There are different types of norm calculations which are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d6dd8-729b-4537-b9f3-20ac9dee31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Norms\n",
    "# L1 norm/ Absolute norm/ Taxicab Norm/ Manhattan Norm\n",
    "# It varies linearly at all locations in space i.e its useful when difference between zero and non-zero is key\n",
    "x= np.array([25,2,5])\n",
    "l1= np.abs(25)+ np.abs(2) + np.abs(5)\n",
    "print(f\"L1 norm is {l1}\")\n",
    "\n",
    "# L2 norm/ Root square norm/ Euclidean norm (Most Popular)\n",
    "# It calculates the euclidean distance of the vector from the origin\n",
    "x= np.array([25,2,5])\n",
    "l2= (25**2 + 2**2 + 5**2)**0.5\n",
    "print(f\"L2 norm is {l2}\")\n",
    "l2= np.linalg.norm(x)\n",
    "print(f\"L2 norm is {l2}\")\n",
    "\n",
    "# Squared L2 norm\n",
    "# It is equivalent to getting the dot product between the transpose of x and itself i.e xT.x\n",
    "# It is computationally cheaper since it doesn't involve root calculation\n",
    "# It is easily differentiable since calculation of element x requires that element only and not root over all elements\n",
    "x= np.array([25,2,5])\n",
    "sl2= (25**2 + 2**2 + 5**2)\n",
    "print(f\"Squared L2 norm is {sl2}\")\n",
    "sl2= np.dot(x.T,x)\n",
    "print(f\"Squared L2 norm is {sl2}\")\n",
    "\n",
    "# Max norm/ L-infinity norm\n",
    "# It takes maximum of the absolute values of each individual element\n",
    "x= np.array([25,2,5])\n",
    "lmax=np.max([np.abs(25), np.abs(2), np.abs(5)])\n",
    "print(f\"LMax norm is {lmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e20c5-27a5-4853-872a-7919f146add0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vector Regularization\n",
    "\n",
    "Regularization or cost function regularization refers to the process of adding a norm-based penalty term to the cost function to control the values of model parameters/features during model training. Depending on the type of norm used in the penalty, there are different types of regularization.  \n",
    "\n",
    "\n",
    "**L1/ Lasso Regression**:  When $||\\beta||_1$ is added as the penalty term, its called lasso regression.  Since Lasso uses the absolute values of the coefficients, it has the ability to shrink some coefficients to exactly zero, effectively performing feature selection. This makes Lasso regression useful when you want a sparse model that selects only the most important features.\n",
    "\n",
    "**L2/ Ridge Regression**: When $||\\beta||_2$ is added as penalty, its called ridge regression. Since the penalty is based on the squared values of the coefficients, ridge tends to shrink the coefficients but does not drive any coefficients to zero. As a result, all features generally remain in the model, but their effect is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f41b8-f6fe-474c-b0be-d2ddfd2d808c",
   "metadata": {},
   "source": [
    "#### Orthogonal Vectors\n",
    "\n",
    "Any two vectors can be considered orthogonal if they are at 90 degrees to each other. In terms of vector operations, their transpose dot product is zero i.e  $x^T.y=0$. \n",
    "\n",
    "- For any n-dimensional vector space, there are a maximum n orthogonal vectors (assuming non-zero norms).\n",
    "- Orthonormal vectors are orthogonal and all have a unit L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b25dba-6941-488d-93c0-430d0d0313d1",
   "metadata": {},
   "source": [
    "#### Basis Vectors\n",
    "\n",
    "The basis vectors for a vector space indicate the set of vectors such that any other vector in that space can be uniquely represented as a linear combination of these vectors by scaling and adding.\n",
    "\n",
    "Some features of basis vectors: \n",
    "\n",
    "- All basis vectors must be linearly independent\n",
    "- The combination of basis vectors must span the whole vector space\n",
    "  \n",
    "Typically, the basis vectors are n orthonormal vectors along the n axes of a n-dimensional space though there can be other basis vectors too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d349b5e-179c-49d4-8084-586361833f2e",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "It is two dimensional array of numbers which are denoted in uppercase $X$\n",
    "\n",
    "- The notation of matrix shape is in form of (rows, columns)\n",
    "- The n th row can be accessed with $X_{n,:}$\n",
    "- The nth column can be accessed with $X_{:,n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a6e695-b035-43c0-84dd-a98edce64157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X is 6\n",
      "The shape of X is (3, 2)\n",
      "The 1st row of X is [25  2]\n",
      "The 1st column of X is [25  5  3]\n"
     ]
    }
   ],
   "source": [
    "X= np.array ([[25,2],[5,26],[3,7]])\n",
    "print(f\"The size of X is {X.size}\")\n",
    "print(f\"The shape of X is {X.shape}\")\n",
    "print(f\"The 1st row of X is {X[0,:]}\")\n",
    "print(f\"The 1st column of X is {X[:,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccee103-19ff-4859-9715-b7c81412929a",
   "metadata": {},
   "source": [
    "#### Matrix Spaces\n",
    "\n",
    "There are two important aspects of a matrix that determine certain important properties of the matrix. They are: \n",
    "\n",
    "1. Row Space\n",
    "2. Column Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fc65c-4638-4e03-89f2-8b427aa35e14",
   "metadata": {},
   "source": [
    "##### Row Space\n",
    "\n",
    "The row space of a matrix indicates that the span of its row vectors i.e all possible linear combinations of the row vectors. \n",
    "\n",
    "- It gives an insight into the relations between equations in a system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac646e-2fc6-4783-b68d-2d81293ae5c8",
   "metadata": {},
   "source": [
    "##### Column Space\n",
    "\n",
    "The column space of a matrix indicates the span of its column vectors i.e all possible linear combinations of the column vectors. \n",
    "\n",
    "- It represents all the vectors that can be reached by the linear transformation defined by the matrix. For a matrix A, when we multiply with vector x then $Ax$ lies in the columnspace of A.\n",
    "- Because of this the columspace is the range or image of the matrix since it shows where the matrix will send any input from its domain.\n",
    "- For $Ax=B$ to have a solution, B must lie in the column space of A or the system has no solutions.\n",
    "\n",
    "Both row space and column space are helpful in understanding different aspects of a matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832bfe4-d802-4afd-b2bf-eb2ee1becdd3",
   "metadata": {},
   "source": [
    "#### Special Matrices\n",
    "\n",
    "There are some special matrices which have fixed properties: \n",
    "\n",
    "1. Symmetric Matrix\n",
    "2. Identity Matrix\n",
    "3. Inverse Matrix\n",
    "4. Diagonal Matrix\n",
    "5. Orthogonal Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed0f00-265c-4f7e-8ad6-63f2911d3a5b",
   "metadata": {},
   "source": [
    "##### Symmetric Matrix\n",
    "\n",
    "A symmetric matrix is one which is square (same number of rows and columns) and $X^T=X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61177963-2588-4e58-b696-5f553d36a42f",
   "metadata": {},
   "source": [
    "##### Identity Matrix\n",
    "\n",
    "An identity matrix is a symmetric matrix where: \n",
    "\n",
    "1. Every element along main diagonal is 1 and all other elements are zero.\n",
    "2. The identity matrix is same for all matrices of the same dimension i.e all $4 \\times 4$ matrices have the identity matrix $I_4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2522ba-7c55-46fe-9dc7-05becd2c8688",
   "metadata": {},
   "source": [
    "##### Inverse Matrix\n",
    "\n",
    "The inverse of a matrix is a matrix whose dot product with the original matrix gives the identity matrix. If A is a matrix and $A^{-1}$ is its inverse matrix then\n",
    "\n",
    "$$ AA^{-1}= I_A$$\n",
    "\n",
    "The conditions for matrix inverse to exist are are: \n",
    "\n",
    "- They are non-singular i.e they have linearly independent rows and thus have intersecting graphs instead of overlap or parallel.\n",
    "- They are square i.e vector range (rowspace)= vector span (columnspace) which avoids overdetermination ($n_{equations}>n_{dimensions}$) or underdetermination ($n_{equations}<n_{dimensions}$)\n",
    "\n",
    "The inverse of a matrix is important because it helps in in determining solutions of matrix equations of the form $AX=B$ where X is solved by $A^{-1}B$ if the inverse exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fcd76-3bad-428e-b4fd-6f050e94360e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Diagonal Matrix\n",
    "\n",
    "The diagonal matrices have non-zero elements along main diagonal and null elements everywhere else. They are computational efficient where multiplication and inversion can be derived via pointwise operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0f5b7-25cd-40ce-b59d-4b1864d6dd2c",
   "metadata": {},
   "source": [
    "##### Orthogonal Matrix\n",
    "\n",
    "The orthogonal matrice have orthonormal vectors in all rows and columns. Thus, for an orthogonal matrix A, \n",
    "\n",
    "$$A^TA= AA^T= I$$\n",
    "$$A^T= A^{-1}I= A^{-1}$$\n",
    "\n",
    "This makes orthogonal matrices computationally efficient as calculating $A^T$ is cheap and for such matrices calculating $A^{-1}$ also becomes cheap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de27dd4-d934-49f1-b4bd-a53284c0a685",
   "metadata": {},
   "source": [
    "#### Eigenconcept\n",
    "\n",
    "We talk of eigens of a transformation as those elements in the columspace of a transformation which remain unchanged. When talking of eigenconcepts, there are two important parts: \n",
    "\n",
    "- Eigenvectors are those vectors whose orientation remain unchanged with a transformation\n",
    "- Eigenvalues are the values by which the eigenvectors change length with the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a22484-ca4b-4929-8823-ac68e48b9e0d",
   "metadata": {},
   "source": [
    "#### Features of Matrices\n",
    "\n",
    "There are some features of matrices that give us important information about a matrix that is useful for further operations: \n",
    "\n",
    "1. Matrix Rank\n",
    "2. Matrix Norm\n",
    "3. Matrix Kernel\n",
    "4. Matrix Determinant\n",
    "5. Trace Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd9b44-9663-4739-b820-1cc8194303f9",
   "metadata": {},
   "source": [
    "##### Matrix Rank\n",
    "\n",
    "The rank of a matrix is the number of linearly independent rows in a matrix and indicates the amount of information that the matrix contains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee310f-ab6a-4f58-b74a-01e875d294e1",
   "metadata": {},
   "source": [
    "##### Matrix Norm \n",
    "\n",
    "Also known as the Frobenius norm, it gives the norm of a matrix: \n",
    "\n",
    "$$ ||X||_F= \\sqrt\\sum_{i,j}X_{i,j}^2 $$\n",
    "\n",
    "This norm is analogous to the L2 vector norm and measures the size of matrix in terms of Euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a266e2-6ddf-4beb-a836-6fd4b552f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2426)\n"
     ]
    }
   ],
   "source": [
    "X= torch.tensor([[1.,2.],\n",
    "                [2.,3.]])\n",
    "print(torch.norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4cee6-479f-485d-a680-c101a07ff509",
   "metadata": {},
   "source": [
    "##### Matrix Kernel\n",
    "\n",
    "The kernel or null space of a matrix consists of all vectors x such that $Ax=0$ i.e when a vector undergoes the transformation defined by the matrix, the null space is the set of all points that are transformed to the origin. \n",
    "\n",
    "The null space indicates whether a matrix has non trivial solutions to the homogenous equation and is closely related to rank by the rank-nullity theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97284beb-7d2a-4e14-b0ed-732614852af2",
   "metadata": {},
   "source": [
    "##### Matrix Determinant\n",
    "\n",
    "The determinant of a matrix is a scalar value that helps determine whether an matrix is invertible.\n",
    "\n",
    "- Zero Determinant: It means the matrix is not invertible/ singular\n",
    "- Non-zero Determinant: It means the matrix is invertible and that its rows/columns are linearly independent\n",
    "\n",
    "Determinants are also useful in solving linear equations with Cramer's rule and understanding permitted geometric transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673dbe0-1057-4a40-b398-3ef47dd148ec",
   "metadata": {},
   "source": [
    "##### Trace Operator\n",
    "\n",
    "The trace operator of a mtrix is the sum of all diagonal elements\n",
    "\n",
    "$$Tr(A)= \\sum A_{i,j} \\text{where i=j}$$\n",
    "\n",
    "Thus trace of a matrix and its transpose are the same. It is also useful in calculating the Frobenius Norm of a matrix\n",
    "\n",
    "$$||A_F||= \\sqrt(Tr(AA^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb89ff2-05ef-4126-9585-f20428be7a6f",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are higher dimensional arrays which are commonly used to represent real world data. \n",
    "\n",
    "Example: Images in a training set for a model are 4-dimension tensors:\n",
    "\n",
    "- Dim 1: Number of images in a training batch eg 32\n",
    "- Dim 2: Image height in pixels eg 28\n",
    "- Dim 3: Image width in pixels eg 28\n",
    "- Dim 4: Number of color channels eg 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80828b8f-15d9-402d-ad28-b605c4a7f752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images= torch.zeros([32,28,28,3])\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06a34d-0bca-46a8-a7dc-8f73303f5129",
   "metadata": {},
   "source": [
    "## Common Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5de962-61cb-4c6e-b273-bfe2a11313cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scalar Operations\n",
    "\n",
    "Both scalar addition and multiplication applies to all elements in a tensor while retaining tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8876064-5515-4ba7-9051-d2cedd219b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n",
      "[[ 4  6  8]\n",
      " [10 12 14]]\n",
      "[[ 4  8 12]\n",
      " [16 20 24]]\n"
     ]
    }
   ],
   "source": [
    "X= np.array([[2,4,6],\n",
    "             [8,10,12]])\n",
    "print(X)\n",
    "print(X+2) #torch.add\n",
    "print(X*2) #torch.mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a42b-6c46-4d3b-b687-faf8dc61d037",
   "metadata": {},
   "source": [
    "### Elementwise Operations\n",
    "\n",
    "For two tensors with same size, we can add them together (elementwise addition) or multiply corresponding elements (elementwise product/Hadamard product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45be32a3-3bd1-441f-ac32-c5a7443b5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "[[ -1  -4  -9]\n",
      " [-16 -25 -36]]\n"
     ]
    }
   ],
   "source": [
    "X= np.array([[1,2,3],\n",
    "             [4,5,6]])\n",
    "Y= np.array([[-1,-2,-3],\n",
    "             [-4,-5,-6]])\n",
    "print(X+Y)\n",
    "print(X*Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e546e-1fcf-4757-bc59-14992ad4c023",
   "metadata": {},
   "source": [
    "### Reduction Operations\n",
    "\n",
    "For any given tensor, we can reduce it by summing across all elements or multiplying across all elements. The sum reduction is most common and can be represented as follows for a 2-D matrix X with $m \\times n$ dimensions: \n",
    "\n",
    "$$ \\sum_{i=1}^m \\sum_{i=1}^n X_{i,j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323fdc08-54f0-4a03-8704-ec5f7a33ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[5 7 9]\n",
      "[ 6 15]\n"
     ]
    }
   ],
   "source": [
    "X= np.array([[1,2,3],\n",
    "             [4,5,6]])\n",
    "print(X.sum())\n",
    "print(X.sum(axis=0)) #sum all rows\n",
    "print(X.sum(axis=1)) #sum all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d3745-2517-44ba-bacb-c2beab1c37a8",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "The dot product of two vectors with same length is represented by $x.y$ or $x^Ty$ which is calculated as follows: \n",
    "\n",
    "$$ x.y = \\sum_{i=1}^n x_i . y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f139be3-ffae-4240-91cf-93bd5cc29486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x=np.array([1,2,3])\n",
    "y=np.array([1,1,1])\n",
    "print(np.dot(x,y)) #also torch.dot if elements are float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116261e0-c382-48dc-9b11-a2de98db21cf",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "It is one of the most important types of tensor operations and involves dot products between rows of one matrix and columns of the other. It can be defined as\n",
    "\n",
    "$$ C _{i,k}= \\sum_j A_{i,j} B_{j,k} $$\n",
    "\n",
    "If we imagine the matrix A to define a transformation, then the matrix multiplication indicates where A takes the matrix B in the new transformed space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c08483-3e73-4a7c-aaa2-19dc2a30241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 12],\n",
      "        [15, 30]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor ([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "B= torch.tensor ([[1,2],\n",
    "                  [1,2],\n",
    "                  [1,2]])\n",
    "C= torch.matmul(A,B)  # in numpy we can still use np.dot for matrix multiplication\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647080a2-c106-40ca-8c86-0f1e6c8a88e8",
   "metadata": {},
   "source": [
    "## Solving Linear Equations\n",
    "\n",
    "There are two main ways of solving linear systems of equations by hand\n",
    "\n",
    "1. Substitution\n",
    "2. Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef578b5b-14fc-45f9-b017-377186c41e07",
   "metadata": {},
   "source": [
    "### Substitution\n",
    "\n",
    "The method involves the following steps: \n",
    "\n",
    "1. Isolate a variable i.e make one variable have a coefficient of 1.\n",
    "2. Substitute this variable into the other equation and get value of the other variable\n",
    "3. Substitute this value into the first equation and get value of the first variable. \n",
    "\n",
    "<img src=\"../images/linalg_sub.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Expii blogpost](https://www.expii.com/t/solving-linear-systems-with-substitution-definition-examples-4412) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df36532-eb52-437d-a73f-9ccf39219c7f",
   "metadata": {},
   "source": [
    "### Elimination\n",
    "\n",
    "The method involves the following steps: \n",
    "\n",
    "1. Add equations together (with our without scalar multiplication) to eliminate one of the variables.\n",
    "2. Solve for the remaining variable\n",
    "3. Solve for the other variable by substituting the previous value in one of the equations.\n",
    "\n",
    "   \n",
    "<img src=\"../images/linalg_elim.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Expii blogpost](https://www.expii.com/t/elimination-by-addition-and-subtraction-examples-practice-4416) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43033b2b-af47-4d8d-bd06-cd713c772a1a",
   "metadata": {},
   "source": [
    "## Applying Linear Equations\n",
    "\n",
    "The theory of linear equations is frequently applied in machine learning and deep learning when the data is represented in the form of vectors/matrices/ tensors. Some common applications are highlighted below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a2610-b913-415b-8176-1e46c6904269",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "In typical linear regression problem, the data can be represented as matrices where: \n",
    "\n",
    "1. Rows correspond to different data points or records in the data\n",
    "2. Columns correspond to multiple coefficients of a parameter\n",
    "\n",
    "In such cases, solving a linear equation system  gives us the values of variables/parameters in the row equations so that we can use them for calculating output values for novel data points. \n",
    "\n",
    "<img src=\"../images/linalg_matrix.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [John Kron's Course](https://learning.oreilly.com/videos/the-essential-machine/9780137903245/9780137903245-LAM1_01_05_05/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0ed98a-e3f3-4aeb-8c66-88c15a323d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  4.]\n"
     ]
    }
   ],
   "source": [
    "# Closed form solution\n",
    "# Let equations be 4b+2c=4 and -5b-3c=-7\n",
    "y= np.array([4,-7])\n",
    "X= np.array ([[4,2],[-5,-3]])\n",
    "# w= inv(X).y\n",
    "X_inv=np.linalg.inv(X)\n",
    "w= np.dot(X_inv,y)\n",
    "print(w) # this gives the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f51166-c674-4b39-8625-179171132fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. -7.]\n"
     ]
    }
   ],
   "source": [
    "# confirming the solution\n",
    "y= np.dot(X,w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83336a-edac-47e4-b84f-0d3aca369498",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks\n",
    "\n",
    "In artificial neural networks, the output of each layer of neurons is calculated in terms  of the following: \n",
    "\n",
    "1. x: The input feature vector\n",
    "2. w: The weights of neurons\n",
    "3. b: The bias of neurons\n",
    "\n",
    "where the output z is: \n",
    "\n",
    "$$ z= wx + b$$\n",
    "\n",
    "Here, the $w$ and $x$ undergo matrix multiplication after which the bias is added. The output $z$ again serves as the input feature vector for the next layer and so it goes on. Thus, we see that even artificial neural networks solve linear equations through their layers. \n",
    "\n",
    "<img src=\"../images/linalg_ann.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Jeremy Jordan's Blog](https://www.jeremyjordan.me/intro-to-neural-networks/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494018cd-f9a6-412b-a4e0-eadd352bb866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
