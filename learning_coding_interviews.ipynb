{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7074118c-3c15-4b9c-9633-5cbb4673aab0",
   "metadata": {},
   "source": [
    "# Concept\n",
    "\n",
    "This notebook contains some common algorithms in ML and DL which are asked in interviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467d4a7-dd99-4193-8ff8-1e5e1b359e83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52435e12-f952-425b-83e7-41fa52e59f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# least squares model\n",
    "class LinearRegressionLS(): \n",
    "    def __init__(self): \n",
    "        self.slope= None\n",
    "        self.intercept= None\n",
    "        \n",
    "    def forward(self,X): \n",
    "        y_pred=[]\n",
    "        for x in X:\n",
    "            y_pred.append((self.slope*x) + self.intercept)\n",
    "        return y_pred\n",
    "            \n",
    "    def fit(self,X,y): \n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        x_mean= torch.mean(X,dim=0)\n",
    "        y_mean= torch.mean(y,dim=0)\n",
    "        num=torch.sum((X-x_mean)* (y-y_mean))\n",
    "        den= torch.sum((X-x_mean)**2)\n",
    "        self.slope= num/den\n",
    "        self.intercept= y_mean- (self.slope*x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd90f4b8-81b4-49ad-808d-019d0d34cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(0.)\n",
      "[tensor(2.), tensor(4.), tensor(6.), tensor(8.), tensor(10.)]\n"
     ]
    }
   ],
   "source": [
    "# least squares test\n",
    "X=torch.tensor([1,2,3,4,5])\n",
    "y=torch.tensor([2,4,6,8,10])\n",
    "lr=LinearRegressionLS()\n",
    "lr.fit(X,y)\n",
    "print(lr.slope)\n",
    "print(lr.intercept)\n",
    "y_pred=lr.forward(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11fb32dc-8c4b-40c5-984b-1d6e79a975e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0056]])\n",
      "tensor([-0.0220])\n",
      "tensor([[ 1.9837],\n",
      "        [ 3.9893],\n",
      "        [ 5.9950],\n",
      "        [ 8.0006],\n",
      "        [10.0063]])\n"
     ]
    }
   ],
   "source": [
    "# gd method\n",
    "class LinearRegressionGD():\n",
    "    def __init__(self,features= 1): \n",
    "        self.w= torch.randn(features,1)\n",
    "        self.b= torch.randn(1)\n",
    "    def forward(self,X): \n",
    "        X = X.float()\n",
    "        return (X@self.w) + self.b\n",
    "    def fit(self, X, y, num_epoch=1000, lr=0.01, regul=0.01): \n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        for epoch in range(num_epoch): \n",
    "            y_pred= self.forward(X)\n",
    "            loss= torch.mean((y-y_pred)**2) +regul*torch.sum(self.w**2)\n",
    "            dw= 2* torch.mean(X.T@ (y_pred-y)) + 2* regul*self.w\n",
    "            db= 2* torch.mean((y_pred-y)) \n",
    "            self.w-= lr*dw\n",
    "            self.b-= lr*db\n",
    "\n",
    "# gd test\n",
    "X=torch.tensor([[1],[2],[3],[4],[5]])\n",
    "y=torch.tensor([[2],[4],[6],[8],[10]])\n",
    "lr=LinearRegressionGD()\n",
    "lr.fit(X,y)\n",
    "print(lr.w)\n",
    "print(lr.b)\n",
    "y_pred=lr.forward(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9bbe3-ddc7-4d01-9211-3963715e7dc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is used to do classification based on regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7d6dd-b72a-4982-b0d1-205b7e2e5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log reg model\n",
    "class LogisticRegressionGD(): \n",
    "    def __init__(self,features=5): \n",
    "        self.w= torch.randn(features, 1)\n",
    "        self.b= torch.randn(1)\n",
    "    def sigmoid(self,z): \n",
    "        return 1/(1+torch.exp(-z))\n",
    "    def forward(self,X): \n",
    "        z= (X@self.w) + self.b\n",
    "        y_pred= self.sigmoid(z)\n",
    "        return y_pred\n",
    "    def fit(self,X,y,num_epochs=1000, lr=0.01, regul=0.01): \n",
    "        y_pred= self.forward(X)\n",
    "        loss = -torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred )) + regul * torch.sum(self.w ** 2)\n",
    "        dw = torch.mean(X.T @ (y_pred-y)) + 2 * regul * self.w  \n",
    "        db = torch.mean((y_pred-y))\n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "\n",
    "# log reg test\n",
    "X = torch.tensor([[1, 0, 0, 0, 1], [1, 1, 1, 0, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1]], dtype=torch.float32)\n",
    "\n",
    "lr = LogisticRegressionGD()\n",
    "lr.fit(X, y, num_epochs=100, lr=0.01, regul=0.01)\n",
    "print(lr.w)\n",
    "print(lr.b)\n",
    "y_pred=lr.forward(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ac6c4c35-7fef-4eb2-a653-dd81bd0ff813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3114],\n",
      "        [ 0.8365],\n",
      "        [ 0.2881],\n",
      "        [-0.9445],\n",
      "        [-0.6534]])\n",
      "tensor([0.0890])\n",
      "tensor([[0.2941],\n",
      "        [0.5619]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54996c0-7f21-4b3d-9d13-d1102c41dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM from scratch\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d642be0-ccc0-48e0-9526-32327e10a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(): \n",
    "    def __init__(self,lr, n_iters,lambdaparam ): \n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    def predict(self,X): \n",
    "        approx= X@self.w- self.b\n",
    "        return torch.sign(approx)\n",
    "    def fit(X,y): \n",
    "        ns,nf=X.shape\n",
    "        y_= np.where(y<=0,-1,1)\n",
    "        self.w= np.zeros(nf)\n",
    "        self.b=0\n",
    "        for r in range(n_iters): \n",
    "            for idx,xi in enumerate(X): \n",
    "                condition= y_[idx]* ((x_i@self.w)- self.b)>=1\n",
    "                if condition: \n",
    "                    self.w-= self.lr*(2*self.lambdaparam*self.w) \n",
    "                else: \n",
    "                     self.w-= self.lr*(2*self.lambdaparam*self.w-torch.dot(xi,y_[idx]))\n",
    "                     self.b -= self.lr * y_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca1d3b-e558-4656-8868-14a51df7aa2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3bbc42a4-5cd7-465c-bf34-88433318ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[-0.4500],\n",
      "        [-1.0072],\n",
      "        [-1.5644],\n",
      "        [-2.1217]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.Module\n",
    "import torch\n",
    "from torch import nn\n",
    "class Perceptron(nn.Module): \n",
    "    def __init__(self, features=5): \n",
    "        super(Perceptron,self).__init__()\n",
    "        self.w= nn.Parameter(torch.randn(features,1))\n",
    "        self.b=nn.Parameter(torch.randn(1))\n",
    "    def sigmoid(self,z): \n",
    "        return 1/(1+torch.exp(-z))\n",
    "    def forward(self,X): \n",
    "        return (X@self.w) + self.b\n",
    "    def fit(self, X, y,lr=0.01,num_epoch=100): \n",
    "        loss= nn.CrossEntropyLoss()\n",
    "        optimizer= torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        for i in range(num_epoch): \n",
    "            optimizer.zero_grad()\n",
    "            y_pred=self.forward(X)\n",
    "            l= loss(y,y_pred)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "X = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                  [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                  [0.3, 0.6, 0.9, 1.2, 1.5],\n",
    "                  [0.4, 0.8, 1.2, 1.6, 2.0]], dtype=torch.float32)  # shape (4, 5)\n",
    "\n",
    "\n",
    "y = torch.tensor([[0.2], [0.4], [0.6], [0.8]], dtype=torch.float32)  # shape (4, 1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Perceptron(features=5)\n",
    "model.fit(X,y)\n",
    "print(\"Predictions:\", model.forward(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "933c420f-5812-4c98-91eb-de9cd0c1994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[ 0.8814],\n",
      "        [-0.0294],\n",
      "        [ 0.6574],\n",
      "        [ 0.4297],\n",
      "        [-0.0130]])\n",
      "Bias: tensor([-0.9319])\n",
      "Predictions: tensor([[0.3806],\n",
      "        [0.4895],\n",
      "        [0.5993],\n",
      "        [0.7001]])\n"
     ]
    }
   ],
   "source": [
    "# scratch\n",
    "import torch\n",
    "from torch import nn\n",
    "class Perceptron(): \n",
    "    def __init__(self, features= 3): \n",
    "        self.w=torch.randn(features,1)\n",
    "        self.b=(torch.randn(1))\n",
    "    def sigmoid(self,z): \n",
    "        return 1/(1+torch.exp(-z))\n",
    "    def forward(self, X): \n",
    "        z= (X@self.w) + self.b\n",
    "        y_pred=self.sigmoid(z)\n",
    "        return y_pred\n",
    "    def fit(self,X,y,reg_strength=0.01, num_epochs=100, lr=0.1):\n",
    "        for i in range (num_epochs): \n",
    "            y_pred=self.forward(X)\n",
    "            loss= torch.mean((y-y_pred)**2) + reg_strength*torch.sum(self.w**2)\n",
    "            dw= 2*torch.mean(X.T@(y_pred-y)) + 2*reg_strength*self.w\n",
    "            db= 2*torch.mean(y_pred-y)\n",
    "            self.w-=lr*dw\n",
    "            self.b-=lr*db\n",
    "        return self.w,self.b\n",
    "\n",
    "X = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                  [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                  [0.3, 0.6, 0.9, 1.2, 1.5],\n",
    "                  [0.4, 0.8, 1.2, 1.6, 2.0]], dtype=torch.float32)  # Shape (4, 5)\n",
    "\n",
    "\n",
    "y = torch.tensor([[0.2], [0.4], [0.6], [0.8]], dtype=torch.float32)  # Shape (4, 1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Perceptron(features=5)\n",
    "w, b = model.fit(X, y)\n",
    "print(\"Weights:\", w)\n",
    "print(\"Bias:\", b)\n",
    "print(\"Predictions:\", model.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd8823-6854-49e3-9178-2f3dda5e6443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80965721-b2bd-4e93-b43e-3fe92fb14809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing self attention\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class SelfAttention(nn.Module): \n",
    "    def __init__(self,d,dk,dq,dv): \n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d= d\n",
    "        self.dk= dk\n",
    "        self.dq= dq\n",
    "        self.dv= dv\n",
    "        self.wk=nn.Parameter(torch.randn(d,dk))\n",
    "        self.wq=nn.Parameter(torch.randn(d,dq))\n",
    "        self.wv= nn.Parameter(torch.randn(d,dv))\n",
    "    def forward(self,X): \n",
    "        K= X@self.wk\n",
    "        Q= X@self.wq\n",
    "        V= X@self.wv\n",
    "        attention_score= (Q@K.T)/math.sqrt(self.dk)\n",
    "        attention_softmax= F.softmax(attention_score, \n",
    "                                     dim=-1)\n",
    "        attention_output= attention_softmax@V\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb253189-cf39-47c8-bfbc-a89301716fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792,  0.7671, -1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  0.4954,  0.2692, -0.0770, -1.0205],\n",
      "        [-0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010],\n",
      "        [ 0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255],\n",
      "        [-0.6315, -2.8400, -1.3250,  0.1784, -2.1338,  1.0524]])\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0.2067, -0.7902, -0.5953],\n",
      "        [-7.2397,  4.2180,  1.3796],\n",
      "        [-7.1074,  4.1420,  1.3509],\n",
      "        [ 2.4532, -1.3562, -1.6268],\n",
      "        [-3.6695,  1.8552,  1.2945],\n",
      "        [ 4.7616, -0.3846, -0.4100]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# testing self attention\n",
    "# preparing input\n",
    "seq= \"The lazy brown cat is sleeping\"\n",
    "counts= {w:i for i,w in enumerate(seq.split())}\n",
    "emb= [counts[k] for k,v in counts.items()]\n",
    "emb=torch.tensor(emb)\n",
    "# converting input to embedding\n",
    "d= 6\n",
    "vocab_size= 50000\n",
    "torch.manual_seed(123)\n",
    "embedder= nn.Embedding(vocab_size,d)\n",
    "embedding= embedder (emb).detach()\n",
    "print(embedding)\n",
    "# apply attention on embedding\n",
    "attention=SelfAttention(d,dk= 3,dq=3,dv=3)\n",
    "attention_output= attention(embedding) \n",
    "print(attention_output.shape)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d188b4-676a-4ed1-84a1-863b6b0a379a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9c927946-192d-4438-afed-6fb3775aa46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "class MLPNet(nn.Module): \n",
    "    def __init__(self,l,d, dff): \n",
    "        super(MLPNet,self).__init__()\n",
    "        self.l= l\n",
    "        self.d= d\n",
    "        self.dff= dff\n",
    "        self.w1= nn.Parameter(torch.randn(d,dff))\n",
    "        self.b1= nn.Parameter(torch.randn(l,dff))\n",
    "        self.w2= nn.Parameter(torch.randn(dff,d))\n",
    "        self.b2= nn.Parameter(torch.randn(l,d))\n",
    "    def forward(self,X): \n",
    "        l1= torch.matmul(X, self.w1) + self.b1\n",
    "        activation= torch.relu(l1)\n",
    "        l2= torch.matmul(activation, self.w2) + self.b2\n",
    "        return l2\n",
    "    def fit(self, X,y,num_epochs, lr): \n",
    "        loss= nn.CrossEntropyLoss()\n",
    "        optim= torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        total_loss= 0\n",
    "        for i in range (num_epochs): \n",
    "            optim.zero_grad()\n",
    "            y_pred= self. forward(X)\n",
    "            l= loss(y,y_pred)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            lossf = l.item()\n",
    "        return lossf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ed3148fe-ccf7-4bc4-aae6-b45aacee0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss after Training: -470.34783935546875\n",
      "Predictions: tensor([[-69.8376, -51.5547, -37.4813, -49.0123, -52.6030],\n",
      "        [-57.8359, -64.1302, -46.9399, -63.2226, -56.0338],\n",
      "        [-76.8198, -87.9744, -43.9885, -72.2152, -81.4764],\n",
      "        [-25.5472, -18.7814, -12.4719, -13.7000, -17.0247]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l, d, dff = 4, 5, 10  # Example dimensions\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.randn(l, d)  \n",
    "y = torch.randn(l, d)  \n",
    "\n",
    "# Initialize the model\n",
    "model = MLPNet(l, d, dff)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "avg_loss = model.fit(X, y, num_epochs=num_epochs, lr=learning_rate)\n",
    "\n",
    "# Print results\n",
    "print(\"Average Loss after Training:\", avg_loss)\n",
    "print(\"Predictions:\", model.forward(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44853041-55b5-4a83-998c-2a26bf8babfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
