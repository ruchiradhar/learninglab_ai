{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d637067-f03d-4026-8eb9-6b4c1b32f624",
   "metadata": {},
   "source": [
    "# What is reinforcement learning? \n",
    "\n",
    "In very simple terms, an agent learns to pick actions in an environment to get more reward over time. The point is to make the agent learn the process of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b040fca-d384-4c5e-b1fc-e0f0e95e4348",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "Also known as MDPs, they provide a way to formalize sequential decision making. In an MDP, we have: \n",
    "\n",
    "1. Environment: A surrounding or scenario which defines our world.\n",
    "2. Agent: A model that interacts with the environment it is placed in.\n",
    "3. State: A configuration of the environment that the agent faces at a given time step.\n",
    "4. Action: A step taken by the agent to bring about a change of state in its environment.\n",
    "5. Reward: A score given to the agent as a result of that action.\n",
    "\n",
    "The whole process repeats over and over, referred to as a trajectory, where the agent's aim is to  maximize cumulative reward i.e the reward it gets across time steps (not just at one time step). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8162f05-f4a4-4ba7-8331-ffba6a148aa1",
   "metadata": {},
   "source": [
    "### Formal Problem Notation\n",
    "\n",
    "In an MDP, we have set of states **S** , a set of actions **A**, and a set of rewards **R**. Each of them has a finite set of elements. \n",
    "\n",
    "1. At each time step *t*= **0,1,2,...**, agent is at a state $S_t \\in S$ and takes an action $A_t \\in A$. So, for each time step, we have a state-action pair $(S_t, A_t)$.\n",
    "2. At $S_{t+1}$, we get numerical reward $R_{t+1} \\in R$.\n",
    "3. The reward can be expressed as a function of state and time.\n",
    "   $$f(S_t,A_t)= R_{t+1}$$\n",
    "4. Th trajectory is a sequence of $S, A, R$.\n",
    "   $$ S_0, A_0, R_1, S_1, A_1, R_2, ...$$\n",
    "5. Given **S** and **R** are finite, the random variables $S_t$ and $R_t$ have well defined probability distributions. The transition probability of going from action $a$ in state $s$ to state $s'$ with reward $r$ can be defined.\n",
    "   $$ p(s',r|s,a)= Probability(S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145a250-cd9a-42cc-891e-623510c41f28",
   "metadata": {},
   "source": [
    "### Expected Return\n",
    "\n",
    "In an MDP, the main driving force behind the agent is maximizing it's cumulative reward, which we also know as *Expected Return*. We can define it as \n",
    "\n",
    "$$G_t= R_{t+1)+ R_{t+2)+ R_{t+3)+ ...+ R_T$$, \n",
    "\n",
    "where *T* is the total number of time step and the goal of the agent is to maximize $G$.\n",
    "\n",
    "However, there is an issue if we just add up rewards forever:\n",
    "\n",
    "1. **Infinity problem**: If episodes donâ€™t end , the sum may diverge (e.g., robot that keeps getting +1 reward each step = infinite return). \n",
    "\n",
    "2. **Unrealistic planning**: We care more about sooner rewards than way in the future. For example, a robot that delivers coffee in 10 steps is better than one that delivers in 1000 steps, even if both eventually succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593204ed-7efb-4f4a-b7b2-719001453c5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc2222-29ce-49d5-a708-85e5eb9fdd75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
