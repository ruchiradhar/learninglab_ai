{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d31f59-3b90-4c7a-8a72-e94caa2f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700cf76-5bd3-4506-abf9-52341e852cbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) are the base on which all of deep learning resides. Inspired by the neuronal architecture of the brain, ANNs consist of neurons arranged in layers and each ANN can contain many such layers of neurons through which the input is passed to convert it into the output. \n",
    "\n",
    "There are two main aspects of an ANN architecture we need to understand- the neuron (perceptron) and the network (multilayer perceptron). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b3680-7a43-4a1d-b8ea-2addf672b808",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The individual neuron, also known as the perceptron, comes with a vector of weights/features $w$ and a bias/intercept $b$. \n",
    "\n",
    "- Input: The input is in the form of a vector $x$ where each element indicates the coefficient for a feature\n",
    "- Output: The output is calculated by using the inputs $x$ and the neuron's weight $w$ and bias $b$ and then passing through an activation function $g$.\n",
    "\n",
    "$$ \\hat{y}= g(w.x + b) $$ \n",
    "\n",
    "<img src=\"images/ann.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Medium Blog](https://towardsdatascience.com/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9a5019-1dbd-401f-8dc0-23eceb26fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([0.8176], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([0.6225], grad_fn=<MulBackward0>).\n"
     ]
    }
   ],
   "source": [
    "# Implementing a perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class Perceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Perceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.tensor ([1.,0.,0.,0.,0.],dtype=torch.float16)) #setting weight\n",
    "        self.b=nn.Parameter(torch.tensor ([0.5])) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.dot(w.T,x)+b\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([1.,0.,0.,0.,0.],dtype=torch.float16)\n",
    "X2= torch.tensor([0.,0.,1.,0.,0.],dtype=torch.float16)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=Perceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef344f-04ec-45f1-8ab6-57dded75d000",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "A multilayer perceptron is the typical ANN. It consists of multiple neurons organized in a layer and those layers combine sequentially into a network. The passing of information through layers is known as forward propagation. \n",
    "\n",
    "The forward propagation for one layer of a multilayer perceptron is as follows: \n",
    "\n",
    "- Each input vector $x_{a\\times 1}$ gets sent to different neurons in a layer, each of which have a weight for each input dimension resulting a weight vector $w_{a\\times 1}$\n",
    "\n",
    "$$ \\hat{y}= g(w^T_{1\\times a}x_{a\\times 1} + b) $$ \n",
    "\n",
    "- For a layer with n neurons, the calculation is as follows where \n",
    "\n",
    "$$ \\hat{y}_{n\\times n}= g(w^T_{n\\times a}x_{a\\times n} + b) $$ \n",
    "\n",
    "- In the final output,  each column is the output of the layer for each input.\n",
    "\n",
    "<img src=\"images/fpmlp.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Avabodha Blog](https://avabodha.in/logistic-regression-and-basics-of-neural-network/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb12d4d7-0733-47c6-af41-34f21d6837fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([[0.2380],\n",
      "        [0.0548],\n",
      "        [0.3006]], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([[0.7495],\n",
      "        [0.1141],\n",
      "        [0.4705]], grad_fn=<MulBackward0>).\n"
     ]
    }
   ],
   "source": [
    "# Implementing a one layer perceptron\n",
    "# only the weight and biases change from single perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class LayerPerceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(LayerPerceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.randn (5,3)) #setting weight\n",
    "        self.b=nn.Parameter(torch.randn (3,1)) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.mm(w.T,x.T)+b #note that X is transposed to convert from row vector to column vector\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([[1.,0.,0.,0.,0.]],dtype=torch.float32)\n",
    "X2= torch.tensor([[0.,0.,1.,0.,0.]],dtype=torch.float32)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=LayerPerceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679fa2d-d271-4751-ae73-3a5a30992023",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46d16b-5d7c-43ae-982a-d4e875025cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural network architecture inspired by the organization of human neural system that have the capability to model sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0154d68-317e-43a4-9207-3680aa27e557",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It is not entirely clear who invented the first computational RNNs but the first reference to “recurrent nets” can be found in McCulloch & Pitts (1943)  and then Rumelhart et al (1985) discuss it more from a machine learning and backpropagation perspective. Elman (1990) also builds on this to then propose a way of implementing dynamic memory for time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467de8a9-dbbe-49e3-9e0b-28cff0321231",
   "metadata": {},
   "source": [
    "## Architecture of RNNs\n",
    "\n",
    "The architecture of RNN is as follows: \n",
    "\n",
    "1. Input Embedding: The input is converted into an embedding\n",
    "2. Hidden State: The input embedding is used to calculate a hidden state\n",
    "3. Output Embedding: The hidden state is converted to the output embedding\n",
    "4. Recursion: Once the first hidden state is calculated for the first token, it is also used as part of calculation to get hidden state for the next tokens. Thus, the inputs change while hidden state keeps getting used giving the impression of being able to unroll a network in time. \n",
    "\n",
    "<img src=\"images/rnn.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e69ad-3702-440f-8147-c2ad9b3b419d",
   "metadata": {},
   "source": [
    "## Features of RNNs\n",
    "\n",
    "Advantage: They could model sequences\n",
    "\n",
    "Disadvantages: \n",
    "\n",
    "- The weights U, V, W remain constant no matter how much the RNN grows which seems restrictive in terms of determining how each word differently impacts the next in  a sequence.\n",
    "- As the hidden states get chnaged with new words, it forgets information about the past. Thus RNNs suffer from memory overloading in longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e540-a64e-481e-94cb-35d4bbbf5286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Long Short Term Memory Network\n",
    "\n",
    "LSTMs are a special type of RNN that were built to overcome the issues of forgetting in vanilla RNNs. \n",
    "\n",
    "Background: \n",
    "\n",
    "Basics: For any new input, vanilla RNNs would completely modify the hidden state which would lead to “forgetting”. To prevent this amnesia, LSTM were modified with 3 gates- forget gate, input gate and output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e6f34-eb2c-4d57-8020-f264660e142a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Variational Autoencoders (VAEs) are generative models that learn to represent high-dimensional data, like images, in a lower-dimensional latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451ca3d-0575-4a4a-b31f-71e0345b6e4d",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It was invented by [Kingma & Welling](https://arxiv.org/abs/1312.6114) (2014). It is part of the families of probabilistic graphical models and variational Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c056-b605-48f2-8da2-49356f3e3cb8",
   "metadata": {},
   "source": [
    "## Architecture of VAEs\n",
    "\n",
    "It consists of the main following parts:\n",
    "\n",
    "**Encoder**: The encoder maps input data (e.g., an image) to a probability distribution in the latent space. Instead of encoding an input to a single point, as in traditional autoencoders, the VAE encoder outputs parameters of a probability distribution (mean and variance) that describe the latent variables. This allows the model to generate a range of outputs rather than a single fixed representation.\n",
    "\n",
    "**Latent Space**: The latent space represents the compressed information from the input data in the form of a probability distribution. This distribution captures the underlying features of the data, allowing for new data generation by sampling from this space.\n",
    "\n",
    "**Decoder**: The decoder takes a sample from the latent space and reconstructs it back into the original data space, attempting to reproduce the input data as closely as possible.\n",
    "\n",
    "<img src=\"images/vae.webp\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [TDS Blog](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cda25-d4b5-4bd3-9877-fd9bcad0bb65",
   "metadata": {},
   "source": [
    "## Features of VAEs\n",
    "\n",
    "1. **Distributional Encoding**: Traditional autoencoders encode data to fixed points in the latent space, while VAEs encode data into distributions\n",
    "   \n",
    "2. **Generative Capability**: VAEs are designed to generate new data points by sampling from the learned latent space distributions, whereas traditional autoencoders primarily focus on reconstructing the input without a generative aspect.\n",
    "\n",
    "3. **Reparameterization Trick**: One of the key innovations in VAEs is the reparameterization trick, which enables efficient backpropagation through the stochastic sampling process. Directly sampling from the latent distribution (mean and variance) introduces randomness that disrupts gradient-based optimization, making it difficult to train the network using backpropagation. The reparameterization trick solves this by expressing the sampled latent variable as a deterministic function of the mean, variance, and a random gaussian noise term. We sample from a standard normal distribution (a fixed distribution of zero mean and unit variance) to represent noise and adjust this sample using the learned mean and variance. This reformulation ensures that the sampling process is differentiable and allows efficient gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923af3b-e486-4932-a24f-23db43e23528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a neural network architecture for natural language processing that form the bases of all foundational models/LLMs today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ac8d7-9594-4998-9e7e-1b1b24cebbc6",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The typical architecture, consisting of an encoder and a decoder, was developed by Vaswani et al (2017) for the purpose of sequence transduction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33cee1-2677-40d3-81c5-f4d8c51359ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Architecture\n",
    "\n",
    " A transformer typically consists of two main parts: \n",
    "\n",
    "1. Encoder:  It takes in input and outputs a matrix representation of that input. For instance, the English sentence “How are you?”\n",
    "2. Decoder: It takes encoder output and iteratively generates an output. In our example, the translated sentence “¿Cómo estás?”\n",
    "\n",
    "The encoder and decoder are themselves made up of many layers with same structure (original paper had 6 layers of each).\n",
    "\n",
    "<img src=\"images/transformer.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datascience Dojo Blog](https://datasciencedojo.com/blog/transformer-models-types-their-uses/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c6661-f3c5-43d3-b8db-59e21e1e3918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embedding Layer \n",
    "\n",
    "Since neural networks cannot directly process words, the input sequence needs to be converted into an embedding matrix which can then be passed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa1054-fd49-44bc-96d9-b97743071b57",
   "metadata": {},
   "source": [
    "Each word in the input sequence is converted into a vector of size $N_{dim}$ and then placed on top of each other into an embedding matrix. If the input has length $N_l$, then the final embedding has dimension $N_l \\times N_{dim} $ where the $N_l$ hyperparameter is set to the length of the longest sequence in our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74969c77-ca97-47a1-af8a-178eec855579",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Since transformers are not sequential like RNNs, the positional information needs to be integrated in the input embedding for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728de681-acc2-4a49-b1b1-647d2a3f19a1",
   "metadata": {},
   "source": [
    "Each position in the input sequence is represented using a combination of various sine and cosine functions where each dimension in a token embedding is represented by unique frequencies. The positional encoding matrix is added to the embedding matrix to create a positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cf2f8-e197-4dc1-b97d-df5a1074b0cd",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "The encoder block consists of many encoder layers where the purpose of each encoder layer is to convert each token into an abstract representation vector that contains all learned information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745e74a-fc87-4030-afe0-1e6f46210bcf",
   "metadata": {},
   "source": [
    "The abstract representation is constructed via the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e5bf-375d-4baf-9b1a-8e3da2e97a2c",
   "metadata": {},
   "source": [
    "#### Multi Head Attention\n",
    "\n",
    "This enables the model to calculate attention scores between tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ed1e5-1f4a-41d9-a95e-c4291fedea0d",
   "metadata": {},
   "source": [
    "\n",
    "1. QKV: Each token vector is converted into 3 vectors via matrix multiplication- the query $Q$, the key $K$ and the value $V$.\n",
    "2. QK Matmul: For each token, we get attention scores for other tokens by $QK^T$ which represents the relevance of query token for one key token. In the attention score matrix, $a_{ij}$ represents the attention that ith token pays to the jth token and each row now represents the scores for one token for all other tokens. \n",
    "3. Scaling: Each scores are then scaled down by dividing them by the square root of the dimension of the query/key vectors. This step is implemented to ensure more stable gradients, as the multiplication of values can lead to excessively large effects.\n",
    "4. Softmax: Subsequently, a softmax function is applied to the adjusted scores to obtain the attention weights. This results in probability values ranging from 0 to 1 in for each token.\n",
    "5. AV Matmul: The attention weights are multiplied by the value vector, resulting in an output vector where each row represents the weighted values of a token across dimensions i.e for first element in row 1 represents the importance of token 1 for itself in dimension 1 , the second element in row 1 represents the importance of token 2 for token 1 in dimension 2, and so on. Thus in this process, only the tokens that had high softmax scores for a given token in a dimension are preserved. \n",
    "\n",
    "\n",
    "<img src=\"images/qkv.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Ketan Doshi Blog](https://ketanhdoshi.github.io/Transformers-Why/)\n",
    "\n",
    "6. Concatenation: The calculations detailed above happens separately across $h$ heads where each head has its own QKV weight matrices representing different subspaces of the input.  In the end, the output of each head is concatenated back together side by side.\n",
    "\n",
    "7. Linear Layer: The concatenated matrix has a different dimension with $N_l \\times N_{dim * h}$ and thus is passed through linear layer i.e a weight matrix of dim $N_{dim * h} \\times N_{dim}$ to project it back to the general space.\n",
    "\n",
    "<img src=\"images/multihead.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Jay Alammar's Blog](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70071e-8d9d-4cd5-9b8c-58203c46d1e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the multihead attention is added to the residual and then undergoes layer normalization for each token row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6b85a-a1f7-42a1-b0e6-7161d91029f5",
   "metadata": {},
   "source": [
    "For an input vector $x$, the Layer Normalization is computed as:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "This residual addition followed by normalization helps stabilize activations during forward pass and gradients during backward pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf762c1-0526-49b6-8abc-df135e51ff92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Feedforward Network\n",
    "\n",
    "The feedforward network consists of two fully connected linear layers with a ReLU activation in between. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5088af-f294-410e-919f-a6cad4018529",
   "metadata": {},
   "source": [
    "It can be mathematically expressed as: \n",
    "\n",
    "$$ FFN(x)= (max (0,(xW_1+b_1)))W_2 + b_2 $$\n",
    "\n",
    "The FFN processes the input from the attention module in the following way: \n",
    "\n",
    "1. **Linear Layer 1**: The input from the attention module $x_{N_{l}\\times N_{dim}}$ is projected to a higher dimensional space by the linear layer with dimensions $N_{FFN} > N_{dim}$ which helps map each token i.e each row into a higher dimensional space.\n",
    "2. **ReLU Activation**: The output of the first linear layer undergoes ReLU activation where we choose the maximum of 0 and output. This introduces non-linearity and helps model emphasise some features or aspects of the input.\n",
    "3. **Linear Layer 2**: The second linear layer is used to project the output back into the $N_{dim}$ space for processing by further layers.\n",
    "\n",
    "One important thing to remember is that each token position undergoes the exact same transformation independently of others and thus each FFN can be said to extract the same information type from each input position in a layer, unlike the attention layer where information is passed between positions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d6175-b21b-45a5-890d-d6136454d09c",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the feedforward network is again added to the residual and then undergoes token wise normalization to stabilise activations and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42788cf8-881f-4923-9e3c-838fe5ccae96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Decoder Block\n",
    "\n",
    "The individual elements of a decoder block are similar to those found in the encoder but the weights of every element in the decoder is optimized for a purpose - to generate the next best token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ac5d0-e5c2-49b5-a1db-af471f706ddd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Decoder Input\n",
    "\n",
    "The input to the decoder comes from two sources:\n",
    "\n",
    "1. **Input Encoding**: The decoder starts with one token- the \"start of sequence\" token, creates an embedding and then a positional encoding vector which then passes through the layers (mostly as the V vector). In successive iterations, the list of vectors gets longer and longer as new generations from the decoder are added to the list.\n",
    "2. **Encoder Encoding**: The output of the encoder block is passed into the Multihead Attention module of each decoder layer where it gets multiplied by the query and key weights to function as the Q and K vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27504fa7-aa24-4784-9020-2d047882c0d1",
   "metadata": {},
   "source": [
    "#### Masked Multihead Attention\n",
    "\n",
    "The masked multihead attention is mainly different from general attention by the addition of a look-ahead mask on the scaled score matrix that zeros out attention scores for each token for all following tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7771dad-3fcf-43ee-b921-b9c91c623c93",
   "metadata": {},
   "source": [
    "1. QKV: The positional encoding is multiplied by weights and turned into the QKV matrices.\n",
    "2. QK Matmul: The attention score matrix is calculated\n",
    "3. Scaling: The scores for each element is scaled down by $\\sqrt(Q)$ \n",
    "4. Masking: The mask matrix, with zeros for each position's current and previous token and a mask for the rest, is added to the scaled scores such that the attention scores for following tokens is masked.\n",
    "\n",
    "<img src=\"images/attmask.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datacamp Blog](https://www.datacamp.com/tutorial/how-transformers-work)\n",
    "\n",
    "5. Softmax: The masked scores are softmaxed.\n",
    "6. AV Matmul: The attention scores are combined with the V matrix to push out tokens that were less important. \n",
    "7. Concatenation: The outputs across all heads are concatenated\n",
    "8. Linear Layer: The concatenated output is brought back to the lower dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716647ec-d1a2-4739-8f51-a88eb8d0b694",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the masked multihead attention is added to the residual and then undergoes layer normalization for each token row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2eda9-eb59-4e3e-9995-fbea8d44ebfe",
   "metadata": {},
   "source": [
    "####  Cross Attention\n",
    "\n",
    "This multihead attention module functions exactly as the one in the encoder layer, with the only different being that the input for Q and K comes from the output of the encoder block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8b9a8-489c-49a5-b11c-8960f796212b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the multihead attention is added to the residual and then undergoes layer normalization for each token row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323862a4-2bfb-4a4c-b857-027dcd0048ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Feedforward Network\n",
    "\n",
    "The output of the cross attention is passed through a FFN similar to the one found in the encoder block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e778d4-33a7-4abd-a926-4d2f59ee76ef",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the feedforward network once again undergoes addition to residual followed by normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54b4e8-dfde-48b0-a968-3d9bfe544017",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Classifier\n",
    "\n",
    "The final linear layer of the model acts as a classifier which prompts a score over each token in the model vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806a45c-5b82-48c2-8adc-277ace9ba6ec",
   "metadata": {},
   "source": [
    "The output of the decoder block, of dimensions $N_l \\times N_{dim}$, is passed through a linear layer, of dimension $N_{dim} \\times N_v$, to get an output of dimensions $N_l \\times N_v$ where each row indicates the scores of each vocab item as next token for that token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65bfe9-4ba3-4d90-9b1c-af118731c247",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Softmax\n",
    "\n",
    "The output of the linear classifier is used to convert the scores from linear classifier into a probability distribution over the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8680d29-6d2e-45d0-8825-74155cdaa0f1",
   "metadata": {},
   "source": [
    "\n",
    "$$s(x_i)= \\frac{e^{x_i}}{\\sum_{j=1}^n e^j}$$\n",
    "\n",
    "\n",
    "<img src=\"images/llineartrans.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datacamp Blog](https://www.datacamp.com/tutorial/how-transformers-work)\n",
    "\n",
    "The softmax is a differentiable function and is used to calculate the cross entropy loss $\\sum_{i=1}^n x_i log(p_i)$ for each token which is propagated backward through the network during the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e935ac-a8af-44cc-8c6a-c4d9a0fb32f9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training of a typical transformer involves the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059f5b1-5ca0-4b20-8cd9-8371fb79126b",
   "metadata": {},
   "source": [
    "\n",
    "1. **Forward Pass of Encoder**: The forward pass of encoder takes in the entire training sequence and generates an encoder output which serves as input to cross-attention of decoder layer.\n",
    "2. **Forward Pass of Decoder**: The forward pass of decoder takes in one token input at a time of the sequence and the encoder output, to generate an output matrix\n",
    "3. **Classification**: The decoder output is used to generate a score vector over the model voacbulary.\n",
    "4. **Softmax**: The softmaxed vector output is obtained for a sequence.\n",
    "5. **Ground Truth Encoding**: For each token in sequence, an equivalent one hot encoding is generated where the only 1 is the correct next token.\n",
    "\n",
    "<img src=\"images/onehot.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Jay Alammar's Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "6. **Loss Calculation**: The loss between these two vectors is calculated in terms of encoding difficulty (Cross-Entropy Loss) or divergence (KL Divergence Loss) from one probability distribution to another. The cross entropy loss is most commonly used since the only difference from KL Divergence is the entropy of P which is constant.\n",
    "\n",
    "$$ H(P,Q)= -\\sum_{x=1}^j P(x_i)log_2(Q(x_i))$$\n",
    "\n",
    "7. **Backpropagation**: The gradients are calculated for all weights in the network and updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269be32-227a-4b53-8369-d5231ba46701",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "For the use of transformers in real world scenarios, the softmax vector is used to output a token from the vocabulary and this is known as decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570c94a-0380-42f5-a878-89b176e75011",
   "metadata": {},
   "source": [
    "There are various decoding methods for LLMs: \n",
    "\n",
    "1. **Greedy Decoding**: The output with highest probability from the softmax is chosen. \n",
    "2. **Beam Search**: Given a hyperparameter k, the algorithm tracks the top $k$ vocabulary tokens and chooses one that has the highest probability with all tokens so far. \n",
    "3. **Nucleus Sampling**: Given a hyperparameter p, the algorithm tracks the tokens with cumulative probability above $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66a46d-e998-414f-bcab-3a35ca237e8c",
   "metadata": {},
   "source": [
    "## Features of Transformers\n",
    "\n",
    "There are two main architectural novelties in the transformer: \n",
    "\n",
    "1. **Parallelization**: Unlike RNNs, the tokens in a sequence do not have to be processed one by one and each token undergoes its own separate paralle flow through a transformer.\n",
    "2. **Attention Mechanism**: The parallelization in part is made possible by the attention mechanism which plays the role of hidden state of RNNs, allowing transformers to keep track of other sequence tokens (past or ahead) that the model processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe182e77-30db-46fe-80a0-73e43a88c36f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mechanistic Interpretability Framework\n",
    "\n",
    "The mechanistic interpretability is a specific framework for looking at the mechanisms in transformer models in terms of operations on the residual stream. The main intuition is to break down the high dimensional models into easily understandable composition of mechanisms/components. \n",
    "\n",
    "<img src=\"images/mechtrans.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e472c-6405-48f5-b9bf-562cd89fb057",
   "metadata": {},
   "source": [
    "### Important Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e8a17-4db1-4f1d-a248-37e1b166c6dd",
   "metadata": {},
   "source": [
    "#### Residual Stream\n",
    "\n",
    "The initial input token encodings which parallely undergo transformations throughout a transformer. All components of a transformer (the token embedding, attention heads, MLP layers, and unembedding) communicate with each other by reading and writing to different subspaces of the residual stream. Rather than analyze the residual stream vectors, it can be helpful to decompose the residual stream into all these different communication channels, corresponding to paths through the model.\n",
    "\n",
    "Features of the residual stream: \n",
    "\n",
    "1. **Linear Structuring**: Any communication to and from the residual stream only happens in terms of linear operations- addition or linear map- thus endowing transformers a great deal of linearity. This also has the consequence that residual stream doesn't have a privileged basis.\n",
    "2. **Selective Flow**: The information flow via the residual stream is selective as the model can \"select\" which layers of the transformers it routes a token through where the selectivity is practically implemented as model weights. \n",
    "\n",
    "\n",
    "Note: Privileged basis (sometimes called a \"preferred basis\") for a set of vectors refers to a particular choice of basis vectors that simplifies calculations, enhances understanding, or aligns with specific properties of the vector space such as the $n$ coordinate vectors in a $\\mathbb{R}^n $ space. In the case of transformers, privileged basis for a set of vectors would be those that enhance interpretability or make calculations easier. Specifically for mechanistic interpretability, the task then is to decompose a model in terms of the components that do have privileged basis (embedding, attention, MLP) where privilege is a spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a32c14-4e43-42d7-a0e3-d7782a2e6792",
   "metadata": {},
   "source": [
    "#### Virtual Weights\n",
    "\n",
    "The linearity of the residual stream means that the amount of connection between any two layer can be quantified as \"virtual weights\" that indicate extent to which the later layer reads the information written by the previous layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554baa3-8021-4ce4-b3a2-245b4438a3b8",
   "metadata": {},
   "source": [
    "#### Superposition\n",
    "\n",
    "Due to the dimensionality difference in the residual stream and other model components leading to bottleneck activations, superposition occurs where each dimension is not a unique interpretable feature (since important features like \"London\" are sparse) and it instead encodes a mix of features.  The model thus finds a balance between trying to encode most features and being able to read them out easily. \n",
    "\n",
    "The high load on residual stream bandwidth that leads to superposition also leads to the memory roles of attention & MLP where they read in information and write out the negative version from the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f5f06-4bbe-4ccb-b975-7c8ce3363e78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Attention Circuits\n",
    "\n",
    "The attention mechanism in transformers can be considered to have the following important features: \n",
    "\n",
    "1. There are two main circuits- QK(which computes relations between tokens) and OV (which computes how each token affects the output if attended to).\n",
    "\n",
    "2. The attention heads are independent and additive.\n",
    "\n",
    "<img src=\"images/atthead.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)   \n",
    "\n",
    "3. The attention heads move information i.e they read information from one token and write it to the residual stream of another token. Within an attention block, the series of multiplications are actually associative and the order doesn't really matter. For example, the $W_{OV}$ can be factorized in any way to get a $W_{O}$ and a $W_{V}$, same goes for $W_{QK}$ though OV and QK are very different functions.\n",
    "4. The composition of attention heads forms induction heads which greatly increase expressivity of transformers. Key and query composition are very different from value composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e49e5-2058-46a6-9a24-65edf97d4820",
   "metadata": {},
   "source": [
    "### Reverse Engineering\n",
    "\n",
    "Using toy attention-only  models, we can analyse characteristic behaviours of transformers: \n",
    "\n",
    "1. **Zero layer Transformers**: They emulate bigram statistics.\n",
    "2. **One layer  Transformers**: They emulate bigram + skipgram statistics. Trigrams are hard to learn because positional encodings only encode before and after and not really individual positional information. \n",
    "3. **Two layer Transformers**: At this stage, the composition of attention Heads across layers leads to formation of *induction heads* which are equivalent to a simple in-context learning algorithms. The formation of these induction heads lead to a turning point for emergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e8b10-4079-45a7-93c9-3e6f66ec184c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
