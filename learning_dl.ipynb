{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d31f59-3b90-4c7a-8a72-e94caa2f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700cf76-5bd3-4506-abf9-52341e852cbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) are the base on which all of deep learning resides. Inspired by the neuronal architecture of the brain, ANNs consist of neurons arranged in layers and each ANN can contain many such layers of neurons through which the input is passed to convert it into the output. \n",
    "\n",
    "There are two main aspects of an ANN architecture we need to understand- the neuron (perceptron) and the network (multilayer perceptron). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b3680-7a43-4a1d-b8ea-2addf672b808",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The individual neuron, also known as the perceptron, comes with a vector of weights/features $w$ and a bias/intercept $b$. \n",
    "\n",
    "- Input: The input is in the form of a vector $x$ where each element indicates the coefficient for a feature\n",
    "- Output: The output is calculated by using the inputs $x$ and the neuron's weight $w$ and bias $b$ and then passing through an activation function $g$.\n",
    "\n",
    "$$ \\hat{y}= g(w.x + b) $$ \n",
    "\n",
    "<img src=\"images/ann.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Medium Blog](https://towardsdatascience.com/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9a5019-1dbd-401f-8dc0-23eceb26fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([0.8176], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([0.6225], grad_fn=<MulBackward0>).\n"
     ]
    }
   ],
   "source": [
    "# Implementing a perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class Perceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Perceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.tensor ([1.,0.,0.,0.,0.],dtype=torch.float16)) #setting weight\n",
    "        self.b=nn.Parameter(torch.tensor ([0.5])) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.dot(w.T,x)+b\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([1.,0.,0.,0.,0.],dtype=torch.float16)\n",
    "X2= torch.tensor([0.,0.,1.,0.,0.],dtype=torch.float16)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=Perceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef344f-04ec-45f1-8ab6-57dded75d000",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "A multilayer perceptron is the typical ANN. It consists of multiple neurons organized in a layer and those layers combine sequentially into a network. The passing of information through layers is known as forward propagation. \n",
    "\n",
    "The forward propagation for one layer of a multilayer perceptron is as follows: \n",
    "\n",
    "- Each input vector $x_{a\\times 1}$ gets sent to different neurons in a layer, each of which have a weight for each input dimension resulting a weight vector $w_{a\\times 1}$\n",
    "\n",
    "$$ \\hat{y}= g(w^T_{1\\times a}x_{a\\times 1} + b) $$ \n",
    "\n",
    "- For a layer with n neurons, the calculation is as follows where \n",
    "\n",
    "$$ \\hat{y}_{n\\times n}= g(w^T_{n\\times a}x_{a\\times n} + b) $$ \n",
    "\n",
    "- In the final output,  each column is the output of the layer for each input.\n",
    "\n",
    "<img src=\"images/fpmlp.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Avabodha Blog](https://avabodha.in/logistic-regression-and-basics-of-neural-network/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb12d4d7-0733-47c6-af41-34f21d6837fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([[0.2380],\n",
      "        [0.0548],\n",
      "        [0.3006]], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([[0.7495],\n",
      "        [0.1141],\n",
      "        [0.4705]], grad_fn=<MulBackward0>).\n"
     ]
    }
   ],
   "source": [
    "# Implementing a one layer perceptron\n",
    "# only the weight and biases change from single perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class LayerPerceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(LayerPerceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.randn (5,3)) #setting weight\n",
    "        self.b=nn.Parameter(torch.randn (3,1)) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.mm(w.T,x.T)+b #note that X is transposed to convert from row vector to column vector\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([[1.,0.,0.,0.,0.]],dtype=torch.float32)\n",
    "X2= torch.tensor([[0.,0.,1.,0.,0.]],dtype=torch.float32)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=LayerPerceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679fa2d-d271-4751-ae73-3a5a30992023",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46d16b-5d7c-43ae-982a-d4e875025cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural network architecture inspired by the organization of human neural system that have the capability to model sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0154d68-317e-43a4-9207-3680aa27e557",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It is not entirely clear who invented the first computational RNNs but the first reference to “recurrent nets” can be found in McCulloch & Pitts (1943)  and then Rumelhart et al (1985) discuss it more from a machine learning and backpropagation perspective. Elman (1990) also builds on this to then propose a way of implementing dynamic memory for time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467de8a9-dbbe-49e3-9e0b-28cff0321231",
   "metadata": {},
   "source": [
    "## Architecture of RNNs\n",
    "\n",
    "The architecture of RNN is as follows: \n",
    "\n",
    "1. Input Embedding: The input is converted into an embedding\n",
    "2. Hidden State: The input embedding is used to calculate a hidden state\n",
    "3. Output Embedding: The hidden state is converted to the output embedding\n",
    "4. Recursion: Once the first hidden state is calculated for the first token, it is also used as part of calculation to get hidden state for the next tokens. Thus, the inputs change while hidden state keeps getting used giving the impression of being able to unroll a network in time. \n",
    "\n",
    "<img src=\"images/rnn.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e69ad-3702-440f-8147-c2ad9b3b419d",
   "metadata": {},
   "source": [
    "## Features of RNNs\n",
    "\n",
    "Advantage: They could model sequences\n",
    "\n",
    "Disadvantages: \n",
    "\n",
    "- The weights U, V, W remain constant no matter how much the RNN grows which seems restrictive in terms of determining how each word differently impacts the next in  a sequence.\n",
    "- As the hidden states get chnaged with new words, it forgets information about the past. Thus RNNs suffer from memory overloading in longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e540-a64e-481e-94cb-35d4bbbf5286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Long Short Term Memory Network\n",
    "\n",
    "LSTMs are a special type of RNN that were built to overcome the issues of forgetting in vanilla RNNs. \n",
    "\n",
    "Background: \n",
    "\n",
    "Basics: For any new input, vanilla RNNs would completely modify the hidden state which would lead to “forgetting”. To prevent this amnesia, LSTM were modified with 3 gates- forget gate, input gate and output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e6f34-eb2c-4d57-8020-f264660e142a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Variational Autoencoders (VAEs) are generative models that learn to represent high-dimensional data, like images, in a lower-dimensional latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451ca3d-0575-4a4a-b31f-71e0345b6e4d",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It was invented by [Kingma & Welling](https://arxiv.org/abs/1312.6114) (2014). It is part of the families of probabilistic graphical models and variational Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c056-b605-48f2-8da2-49356f3e3cb8",
   "metadata": {},
   "source": [
    "## Architecture of VAEs\n",
    "\n",
    "It consists of the main following parts:\n",
    "\n",
    "**Encoder**: The encoder maps input data (e.g., an image) to a probability distribution in the latent space. Instead of encoding an input to a single point, as in traditional autoencoders, the VAE encoder outputs parameters of a probability distribution (mean and variance) that describe the latent variables. This allows the model to generate a range of outputs rather than a single fixed representation.\n",
    "\n",
    "**Latent Space**: The latent space represents the compressed information from the input data in the form of a probability distribution. This distribution captures the underlying features of the data, allowing for new data generation by sampling from this space.\n",
    "\n",
    "**Decoder**: The decoder takes a sample from the latent space and reconstructs it back into the original data space, attempting to reproduce the input data as closely as possible.\n",
    "\n",
    "<img src=\"images/vae.webp\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [TDS Blog](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cda25-d4b5-4bd3-9877-fd9bcad0bb65",
   "metadata": {},
   "source": [
    "## Features of VAEs\n",
    "\n",
    "1. **Distributional Encoding**: Traditional autoencoders encode data to fixed points in the latent space, while VAEs encode data into distributions\n",
    "   \n",
    "2. **Generative Capability**: VAEs are designed to generate new data points by sampling from the learned latent space distributions, whereas traditional autoencoders primarily focus on reconstructing the input without a generative aspect.\n",
    "\n",
    "3. **Reparameterization Trick**: One of the key innovations in VAEs is the reparameterization trick, which enables efficient backpropagation through the stochastic sampling process. Directly sampling from the latent distribution (mean and variance) introduces randomness that disrupts gradient-based optimization, making it difficult to train the network using backpropagation. The reparameterization trick solves this by expressing the sampled latent variable as a deterministic function of the mean, variance, and a random gaussian noise term. We sample from a standard normal distribution (a fixed distribution of zero mean and unit variance) to represent noise and adjust this sample using the learned mean and variance. This reformulation ensures that the sampling process is differentiable and allows efficient gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923af3b-e486-4932-a24f-23db43e23528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a neural network architecture for natural language processing that form the bases of all foundational models/LLMs today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ac8d7-9594-4998-9e7e-1b1b24cebbc6",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The typical architecture, consisting of an encoder and a decoder, was developed by Vaswani et al (2017) for the purpose of sequence transduction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33cee1-2677-40d3-81c5-f4d8c51359ae",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    " A transformer typically consists of two main parts: \n",
    "\n",
    "1. Encoder:  It takes in input and outputs a matrix representation of that input. For instance, the English sentence “How are you?”\n",
    "2. Decoder: It takes encoder output and iteratively generates an output. In our example, the translated sentence “¿Cómo estás?”\n",
    "\n",
    "The encoder and decoder are themselves made up of many layers with same structure (original paper had 6 layers of each.\n",
    "\n",
    "<img src=\"images/transformer.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datascience Dojo Blog](https://datasciencedojo.com/blog/transformer-models-types-their-uses/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c6661-f3c5-43d3-b8db-59e21e1e3918",
   "metadata": {},
   "source": [
    "### Embedding Layer \n",
    "\n",
    "Since neural networks cannot directly process words, the input sequence needs to be converted into an embedding matrix which can then be passed into the model. \n",
    "\n",
    "Each word in the input sequence is converted into a vector of size $N_{dim}$ and then placed side by side into an embedding matrix. If the input has length $N_l$, then the final embedding has dimension $N_{dim} \\times N_l $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74969c77-ca97-47a1-af8a-178eec855579",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Since transformers are not sequential like RNNs, the positional information needs to be integrated in the input embedding for better performance.\n",
    "\n",
    "Each position in the input sequence is represented using a combination of various sine and cosine functions where each dimension in a token embedding is represented by unique frequencies. The positional encoding matrix is added to the embedding matrix to create a positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cf2f8-e197-4dc1-b97d-df5a1074b0cd",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "The encoder block consists of many encoder layers where the purpose of each encoder layer is to convert each token into an abstract representation vector that contains all learned information. The abstract representation is constructed via the following steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e5bf-375d-4baf-9b1a-8e3da2e97a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Multi Head Attention\n",
    "\n",
    "This enables the model to calculate attention scores between tokens. \n",
    "\n",
    "1. QKV: Each token vector is converted into 3 vectors via matrix multiplication- the query $Q$, the key $K$ and the value $V$.\n",
    "2. QK Matmul: For each token, we get attention scores for other tokens by $QK^T$ which represents the relevance of query token for one key token. In the attention score matrix, $a_{ij}$ represents the attention that ith token pays to the jth token and each row now represents the scores for one token for all other tokens. \n",
    "3. Scaling: Each scores are then scaled down by dividing them by the square root of the dimension of the query/key vectors. This step is implemented to ensure more stable gradients, as the multiplication of values can lead to excessively large effects.\n",
    "4. Softmax: Subsequently, a softmax function is applied to the adjusted scores to obtain the attention weights. This results in probability values ranging from 0 to 1 in for each token.\n",
    "5. AV Matmul: The attention weights are multiplied by the value vector, resulting in an output vector where each row represents the weighted values of a token across dimensions. In this process, only the words that present high softmax scores are preserved.\n",
    "\n",
    "\n",
    "<img src=\"images/qkv.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Ketan Doshi Blog](https://ketanhdoshi.github.io/Transformers-Why/)\n",
    "\n",
    "6. Concatenation: The calculations detailed above happen separately across $h$ heads where the both the input encoding and QKV is split across the heads. In the end, the output of each head is concatenated back together.\n",
    "\n",
    "\n",
    "<img src=\"images/multihead.webp\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Ketan Doshi Blog](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70071e-8d9d-4cd5-9b8c-58203c46d1e3",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the multihead attention is added to the residual and then undergoes layer normalization for each token row\n",
    "\n",
    "For an input vector $x$, the Layer Normalization is computed as:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "This residual addition followed by normalization helps stabilize activations during forward pass and gradients during backward pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf762c1-0526-49b6-8abc-df135e51ff92",
   "metadata": {},
   "source": [
    "#### Feedforward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d6175-b21b-45a5-890d-d6136454d09c",
   "metadata": {},
   "source": [
    "#### Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42788cf8-881f-4923-9e3c-838fe5ccae96",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
