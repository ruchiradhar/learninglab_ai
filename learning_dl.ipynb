{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d31f59-3b90-4c7a-8a72-e94caa2f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700cf76-5bd3-4506-abf9-52341e852cbe",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) are the base on which all of deep learning resides. Inspired by the neuronal architecture of the brain, ANNs consist of neurons arranged in layers and each ANN can contain many such layers of neurons through which the input is passed to convert it into the output. \n",
    "\n",
    "There are two main aspects of an ANN architecture we need to understand- the neuron (perceptron) and the network (multilayer perceptron). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b3680-7a43-4a1d-b8ea-2addf672b808",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The individual neuron, also known as the perceptron, comes with a vector of weights/features $w$ and a bias/intercept $b$. \n",
    "\n",
    "- Input: The input is in the form of a vector $x$ where each element indicates the coefficient for a feature\n",
    "- Output: The output is calculated by using the inputs $x$ and the neuron's weight $w$ and bias $b$ and then passing through an activation function $g$.\n",
    "\n",
    "$$ \\hat{y}= g(w.x + b) $$ \n",
    "\n",
    "<img src=\"images/ann.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"400\" \n",
    "        height=\"400\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Medium Blog](https://towardsdatascience.com/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d9a5019-1dbd-401f-8dc0-23eceb26fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([0.8176], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([0.6225], grad_fn=<MulBackward0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/5gg1gj414mn7flh45l9cm7_80000gn/T/ipykernel_66863/1360273634.py:16: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1729084833314/work/aten/src/ATen/native/TensorShape.cpp:3701.)\n",
      "  z=torch.dot(w.T,x)+b\n"
     ]
    }
   ],
   "source": [
    "# Implementing a perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class Perceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Perceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.tensor ([1.,0.,0.,0.,0.],dtype=torch.float16)) #setting weight\n",
    "        self.b=nn.Parameter(torch.tensor ([0.5])) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.dot(w.T,x)+b\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([1.,0.,0.,0.,0.],dtype=torch.float16)\n",
    "X2= torch.tensor([0.,0.,1.,0.,0.],dtype=torch.float16)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=Perceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef344f-04ec-45f1-8ab6-57dded75d000",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "A multilayer perceptron is the typical ANN. It consists of multiple neurons organized in a layer and those layers combine sequentially into a network. The passing of information through layers is known as forward propagation. \n",
    "\n",
    "The forward propagation for one layer of a multilayer perceptron is as follows: \n",
    "\n",
    "- Each input vector $x_{a\\times 1}$ gets sent to different neurons in a layer, each of which have a weight for each input dimension resulting a weight vector $w_{a\\times 1}$\n",
    "\n",
    "$$ \\hat{y}= g(w^T_{1\\times a}x_{a\\times 1} + b) $$ \n",
    "\n",
    "- For a layer with n neurons, the calculation is as follows where \n",
    "\n",
    "$$ \\hat{y}_{n\\times n}= g(w^T_{n\\times a}x_{a\\times n} + b) $$ \n",
    "\n",
    "- In the final output,  each column is the output of the layer for each input.\n",
    "\n",
    "<img src=\"images/fpmlp.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Avabodha Blog](https://avabodha.in/logistic-regression-and-basics-of-neural-network/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb12d4d7-0733-47c6-af41-34f21d6837fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for input 1 is tensor([[0.2380],\n",
      "        [0.0548],\n",
      "        [0.3006]], grad_fn=<MulBackward0>).\n",
      "The prediction for input 2 is tensor([[0.7495],\n",
      "        [0.1141],\n",
      "        [0.4705]], grad_fn=<MulBackward0>).\n"
     ]
    }
   ],
   "source": [
    "# Implementing a one layer perceptron\n",
    "# only the weight and biases change from single perceptron\n",
    "\n",
    "# defining the perceptron class\n",
    "class LayerPerceptron(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(LayerPerceptron,self).__init__() #inherit from Module superclass\n",
    "        self.w=nn.Parameter(torch.randn (5,3)) #setting weight\n",
    "        self.b=nn.Parameter(torch.randn (3,1)) #bias\n",
    "        \n",
    "    def sigmoid(self,z):  #defining activation function\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x): #defining a forward pass\n",
    "        w=self.w\n",
    "        b=self.b\n",
    "        z=torch.mm(w.T,x.T)+b #note that X is transposed to convert from row vector to column vector\n",
    "        y_hat= self.sigmoid(z)\n",
    "        return y_hat\n",
    "\n",
    "# defining two inputs\n",
    "X1= torch.tensor([[1.,0.,0.,0.,0.]],dtype=torch.float32)\n",
    "X2= torch.tensor([[0.,0.,1.,0.,0.]],dtype=torch.float32)\n",
    "\n",
    "# calling the perceptron\n",
    "perceptron=LayerPerceptron()\n",
    "prediction1= perceptron.forward(X1)\n",
    "print(f\"The prediction for input 1 is {prediction1}.\")\n",
    "prediction2= perceptron.forward(X2)\n",
    "print(f\"The prediction for input 2 is {prediction2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46d16b-5d7c-43ae-982a-d4e875025cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural network architecture inspired by the organization of human neural system that have the capability to model sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0154d68-317e-43a4-9207-3680aa27e557",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It is not entirely clear who invented the first computational RNNs but the first reference to “recurrent nets” can be found in McCulloch & Pitts (1943)  and then Rumelhart et al (1985) discuss it more from a machine learning and backpropagation perspective. Elman (1990) also builds on this to then propose a way of implementing dynamic memory for time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467de8a9-dbbe-49e3-9e0b-28cff0321231",
   "metadata": {},
   "source": [
    "## Architecture of RNNs\n",
    "\n",
    "The architecture of RNN is as follows: \n",
    "\n",
    "1. Input Embedding: The input is converted into an embedding\n",
    "2. Hidden State: The input embedding is used to calculate a hidden state\n",
    "3. Output Embedding: The hidden state is converted to the output embedding\n",
    "4. Recursion: Once the first hidden state is calculated for the first token, it is also used as part of calculation to get hidden state for the next tokens. Thus, the inputs change while hidden state keeps getting used giving the impression of being able to unroll a network in time. \n",
    "\n",
    "<img src=\"images/rnn.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e69ad-3702-440f-8147-c2ad9b3b419d",
   "metadata": {},
   "source": [
    "## Features of RNNs\n",
    "\n",
    "Advantage: They could model sequences\n",
    "\n",
    "Disadvantages: \n",
    "\n",
    "- The weights U, V, W remain constant no matter how much the RNN grows which seems restrictive in terms of determining how each word differently impacts the next in  a sequence.\n",
    "- As the hidden states get chnaged with new words, it forgets information about the past. Thus RNNs suffer from memory overloading in longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e540-a64e-481e-94cb-35d4bbbf5286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Long Short Term Memory Network\n",
    "\n",
    "LSTMs are a special type of RNN that were built to overcome the issues of forgetting in vanilla RNNs. \n",
    "\n",
    "Background: \n",
    "\n",
    "Basics: For any new input, vanilla RNNs would completely modify the hidden state which would lead to “forgetting”. To prevent this amnesia, LSTM were modified with 3 gates- forget gate, input gate and output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e6f34-eb2c-4d57-8020-f264660e142a",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Variational Autoencoders (VAEs) are generative models that learn to represent high-dimensional data, like images, in a lower-dimensional latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451ca3d-0575-4a4a-b31f-71e0345b6e4d",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "It was invented by [Kingma & Welling](https://arxiv.org/abs/1312.6114) (2014). It is part of the families of probabilistic graphical models and variational Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c056-b605-48f2-8da2-49356f3e3cb8",
   "metadata": {},
   "source": [
    "## Architecture of VAEs\n",
    "\n",
    "It consists of the main following parts:\n",
    "\n",
    "**Encoder**: The encoder maps input data (e.g., an image) to a probability distribution in the latent space. Instead of encoding an input to a single point, as in traditional autoencoders, the VAE encoder outputs parameters of a probability distribution (mean and variance) that describe the latent variables. This allows the model to generate a range of outputs rather than a single fixed representation.\n",
    "\n",
    "**Latent Space**: The latent space represents the compressed information from the input data in the form of a probability distribution. This distribution captures the underlying features of the data, allowing for new data generation by sampling from this space.\n",
    "\n",
    "**Decoder**: The decoder takes a sample from the latent space and reconstructs it back into the original data space, attempting to reproduce the input data as closely as possible.\n",
    "\n",
    "<img src=\"images/vae.webp\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"600\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [TDS Blog](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cda25-d4b5-4bd3-9877-fd9bcad0bb65",
   "metadata": {},
   "source": [
    "## Features of VAEs\n",
    "\n",
    "1. **Distributional Encoding**: Traditional autoencoders encode data to fixed points in the latent space, while VAEs encode data into distributions\n",
    "   \n",
    "2. **Generative Capability**: VAEs are designed to generate new data points by sampling from the learned latent space distributions, whereas traditional autoencoders primarily focus on reconstructing the input without a generative aspect.\n",
    "\n",
    "3. **Reparameterization Trick**: One of the key innovations in VAEs is the reparameterization trick, which enables efficient backpropagation through the stochastic sampling process. Directly sampling from the latent distribution (mean and variance) introduces randomness that disrupts gradient-based optimization, making it difficult to train the network using backpropagation. The reparameterization trick solves this by expressing the sampled latent variable as a deterministic function of the mean, variance, and a random gaussian noise term. We sample from a standard normal distribution (a fixed distribution of zero mean and unit variance) to represent noise and adjust this sample using the learned mean and variance. This reformulation ensures that the sampling process is differentiable and allows efficient gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa1054-fd49-44bc-96d9-b97743071b57",
   "metadata": {},
   "source": [
    "Each word in the input sequence is converted into a vector of size $N_{dim}$ and then placed on top of each other into an embedding matrix. If the input has length $N_l$, then the final embedding has dimension $N_l \\times N_{dim} $ where the $N_l$ hyperparameter is set to the length of the longest sequence in our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728de681-acc2-4a49-b1b1-647d2a3f19a1",
   "metadata": {},
   "source": [
    "Each position in the input sequence is represented using a combination of various sine and cosine functions where each dimension in a token embedding is represented by unique frequencies. The positional encoding matrix is added to the embedding matrix to create a positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6b85a-a1f7-42a1-b0e6-7161d91029f5",
   "metadata": {},
   "source": [
    "For an input vector $x$, the Layer Normalization is computed as:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "This residual addition followed by normalization helps stabilize activations during forward pass and gradients during backward pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806a45c-5b82-48c2-8adc-277ace9ba6ec",
   "metadata": {},
   "source": [
    "The output of the decoder block, of dimensions $N_l \\times N_{dim}$, is passed through a linear layer, of dimension $N_{dim} \\times N_v$, to get an output of dimensions $N_l \\times N_v$ where each row indicates the scores of each vocab item as next token for that token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe182e77-30db-46fe-80a0-73e43a88c36f",
   "metadata": {},
   "source": [
    "## Mechanistic Interpretability Framework\n",
    "\n",
    "The mechanistic interpretability is a specific framework for looking at the mechanisms in transformer models in terms of operations on the residual stream. The main intuition is to break down the high dimensional models into easily understandable composition of mechanisms/components. \n",
    "\n",
    "<img src=\"images/mechtrans.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e472c-6405-48f5-b9bf-562cd89fb057",
   "metadata": {},
   "source": [
    "### Important Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e8a17-4db1-4f1d-a248-37e1b166c6dd",
   "metadata": {},
   "source": [
    "#### Residual Stream\n",
    "\n",
    "The initial input token encodings which parallely undergo transformations throughout a transformer. All components of a transformer (the token embedding, attention heads, MLP layers, and unembedding) communicate with each other by reading and writing to different subspaces of the residual stream. Rather than analyze the residual stream vectors, it can be helpful to decompose the residual stream into all these different communication channels, corresponding to paths through the model.\n",
    "\n",
    "Features of the residual stream: \n",
    "\n",
    "1. **Linear Structuring**: Any communication to and from the residual stream only happens in terms of linear operations- addition or linear map- thus endowing transformers a great deal of linearity. This also has the consequence that residual stream doesn't have a privileged basis.\n",
    "2. **Selective Flow**: The information flow via the residual stream is selective as the model can \"select\" which layers of the transformers it routes a token through where the selectivity is practically implemented as model weights. \n",
    "\n",
    "\n",
    "Note: Privileged basis (sometimes called a \"preferred basis\") for a set of vectors refers to a particular choice of basis vectors that simplifies calculations, enhances understanding, or aligns with specific properties of the vector space such as the $n$ coordinate vectors in a $\\mathbb{R}^n $ space. In the case of transformers, privileged basis for a set of vectors would be those that enhance interpretability or make calculations easier. Specifically for mechanistic interpretability, the task then is to decompose a model in terms of the components that do have privileged basis (embedding, attention, MLP) where privilege is a spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a32c14-4e43-42d7-a0e3-d7782a2e6792",
   "metadata": {},
   "source": [
    "#### Virtual Weights\n",
    "\n",
    "The linearity of the residual stream means that the amount of connection between any two layer can be quantified as \"virtual weights\" that indicate extent to which the later layer reads the information written by the previous layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554baa3-8021-4ce4-b3a2-245b4438a3b8",
   "metadata": {},
   "source": [
    "#### Superposition\n",
    "\n",
    "Due to the dimensionality difference in the residual stream and other model components leading to bottleneck activations, superposition occurs where each dimension is not a unique interpretable feature (since important features like \"London\" are sparse) and it instead encodes a mix of features.  The model thus finds a balance between trying to encode most features and being able to read them out easily. \n",
    "\n",
    "The high load on residual stream bandwidth that leads to superposition also leads to the memory roles of attention & MLP where they read in information and write out the negative version from the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f5f06-4bbe-4ccb-b975-7c8ce3363e78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Attention Circuits\n",
    "\n",
    "The attention mechanism in transformers can be considered to have the following important features: \n",
    "\n",
    "1. There are two main circuits- QK(which computes relations between tokens) and OV (which computes how each token affects the output if attended to).\n",
    "\n",
    "2. The attention heads are independent and additive.\n",
    "\n",
    "<img src=\"images/atthead.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Elhage et al 2021](https://transformer-circuits.pub/2021/framework/index.html)   \n",
    "\n",
    "3. The attention heads move information i.e they read information from one token and write it to the residual stream of another token. Within an attention block, the series of multiplications are actually associative and the order doesn't really matter. For example, the $W_{OV}$ can be factorized in any way to get a $W_{O}$ and a $W_{V}$, same goes for $W_{QK}$ though OV and QK are very different functions.\n",
    "4. The composition of attention heads forms induction heads which greatly increase expressivity of transformers. Key and query composition are very different from value composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e49e5-2058-46a6-9a24-65edf97d4820",
   "metadata": {},
   "source": [
    "### Reverse Engineering\n",
    "\n",
    "Using toy attention-only  models, we can analyse characteristic behaviours of transformers: \n",
    "\n",
    "1. **Zero layer Transformers**: They emulate bigram statistics.\n",
    "2. **One layer  Transformers**: They emulate bigram + skipgram statistics. Trigrams are hard to learn because positional encodings only encode before and after and not really individual positional information. \n",
    "3. **Two layer Transformers**: At this stage, the composition of attention Heads across layers leads to formation of *induction heads* which are equivalent to a simple in-context learning algorithms. The formation of these induction heads lead to a turning point for emergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e8b10-4079-45a7-93c9-3e6f66ec184c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
