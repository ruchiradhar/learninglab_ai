{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267415d9-389d-4471-a080-4748a46bd8f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Basics of Transformers\n",
    "\n",
    "Transformers are a neural network architecture for natural language processing that form the bases of all foundational models/LLMs today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e74e47-832d-4a2c-b650-f50765d914db",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The typical architecture, consisting of an encoder and a decoder, was developed by Vaswani et al (2017) for the purpose of sequence transduction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e0c77-f454-4005-895a-9a31809fd2bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Architecture\n",
    "\n",
    " A transformer typically consists of two main parts: \n",
    "\n",
    "1. Encoder:  It takes in input and outputs a matrix representation of that input. For instance, the English sentence “How are you?”\n",
    "2. Decoder: It takes encoder output and iteratively generates an output. In our example, the translated sentence “¿Cómo estás?”\n",
    "\n",
    "The encoder and decoder are themselves made up of many layers with same structure (original paper had 6 layers of each).\n",
    "\n",
    "<img src=\"images/transformer.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"800\" \n",
    "        height=\"800\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datascience Dojo Blog](https://datasciencedojo.com/blog/transformer-models-types-their-uses/)\n",
    "\n",
    "The details of each part of the architecture are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da282090-84b5-479a-ad9a-1c4ea883885c",
   "metadata": {},
   "source": [
    "### Embedding Layer \n",
    "\n",
    "Since neural networks cannot directly process words, the input sequence needs to be converted into an embedding matrix which can then be passed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3c02b-cae0-4e3d-9447-bf92ecb67b84",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Since transformers are not sequential like RNNs, the positional information needs to be integrated in the input embedding for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21155716-5089-46af-a2ef-bf389af52571",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Encoder Block\n",
    "\n",
    "The encoder consists of many encoder layers where the purpose of each encoder layer is to convert each token into an abstract representation vector that contains all learned information.\n",
    "\n",
    "The abstract representation is constructed via: Multi Head Attention + Add & Norm + Feedforward + Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8890d3-19f5-4773-92e6-5883b6a36b58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Multi Head Attention\n",
    "\n",
    "This enables the model to calculate attention scores between tokens. The operations in MHA are as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62be39-ace6-4bfa-b671-ef205623c1af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "1. QKV: Each token vector is converted into 3 vectors via matrix multiplication- the query $Q$, the key $K$ and the value $V$.\n",
    "2. QK Matmul: For each token, we get attention scores for other tokens by $QK^T$ which represents the relevance of query token for one key token. In the attention score matrix, $a_{ij}$ represents the attention that ith token pays to the jth token and each row now represents the scores for one token for all other tokens. \n",
    "3. Scaling: Each scores are then scaled down by dividing them by the square root of the dimension of the query/key vectors. This step is implemented to ensure more stable gradients, as the multiplication of values can lead to excessively large effects.\n",
    "4. Softmax: Subsequently, a softmax function is applied to the adjusted scores to obtain the attention weights. This results in probability values ranging from 0 to 1 in for each token.\n",
    "5. AV Matmul: The attention weights are multiplied by the value vector, resulting in an output vector where each row represents the weighted values of a token across dimensions i.e for first element in row 1 represents the importance of token 1 for itself in dimension 1 , the second element in row 1 represents the importance of token 2 for token 1 in dimension 2, and so on. Thus in this process, only the tokens that had high softmax scores for a given token in a dimension are preserved. \n",
    "\n",
    "\n",
    "<img src=\"images/qkv.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Ketan Doshi Blog](https://ketanhdoshi.github.io/Transformers-Why/)\n",
    "\n",
    "6. Concatenation: The calculations detailed above happens separately across $h$ heads where each head has its own QKV weight matrices representing different subspaces of the input.  In the end, the output of each head is concatenated back together side by side.\n",
    "\n",
    "7. Linear Layer: The concatenated matrix has a different dimension with $N_l \\times N_{dim * h}$ and thus is passed through linear layer i.e a weight matrix of dim $N_{dim * h} \\times N_{dim}$ to project it back to the general space.\n",
    "\n",
    "<img src=\"images/multihead.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Jay Alammar's Blog](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560be9d-5b31-498a-b720-5e282fdae6ea",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the multihead attention is added to the residual and then undergoes layer normalization for each token row to stabilise activations and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e5cf9-43dd-4feb-bd19-c00897f16685",
   "metadata": {},
   "source": [
    "#### Feedforward Network\n",
    "\n",
    "The feedforward network consists of two fully connected linear layers with a ReLU activation in between. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28037e4f-3e21-4ea3-8862-b9896f3cbb9e",
   "metadata": {},
   "source": [
    "It can be mathematically expressed as: \n",
    "\n",
    "$$ FFN(x)= (max (0,(xW_1+b_1)))W_2 + b_2 $$\n",
    "\n",
    "The FFN processes the input from the attention module in the following way: \n",
    "\n",
    "1. **Linear Layer 1**: The input from the attention module $x_{N_{l}\\times N_{dim}}$ is projected to a higher dimensional space by the linear layer with dimensions $N_{FFN} > N_{dim}$ which helps map each token i.e each row into a higher dimensional space.\n",
    "2. **ReLU Activation**: The output of the first linear layer undergoes ReLU activation where we choose the maximum of 0 and output. This introduces non-linearity and helps model emphasise some features or aspects of the input.\n",
    "3. **Linear Layer 2**: The second linear layer is used to project the output back into the $N_{dim}$ space for processing by further layers.\n",
    "\n",
    "One important thing to remember is that each token position undergoes the exact same transformation independently of others and thus each FFN can be said to extract the same information type from each input position in a layer, unlike the attention layer where information is passed between positions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a506af2-3c85-4c52-9789-2328ed0b36ca",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the feedforward network is again added to the residual and then undergoes token wise normalization to stabilise activations and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd3021-0429-4101-879b-1b22dca8f24b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Decoder Block\n",
    "\n",
    "The individual elements of a decoder block are similar to those found in the encoder but the weights of every element in the decoder is optimized for a purpose - to generate the next best token. \n",
    "\n",
    "The outputs is constructed iteratively with following steps: Masked Multi Head Attention + Add & Norm +  Cross Attention + Add & Norm + Feedforward + Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b3a89-7511-4209-9208-c8eb337b507b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Masked Multihead Attention\n",
    "\n",
    "The masked multihead attention is mainly different from general attention by the addition of a look-ahead mask on the scaled score matrix that zeros out attention scores for each token for all following tokens. \n",
    "\n",
    "Once we have the scaled $QK$ matrix, the mask matrix-  with zeros for each position's current and previous token and a mask for the rest- is added to the scaled scores such that the attention scores for following tokens is masked. Then, the usual softmax, $V$ multiplication, concatenation across heads and linear layer processing happens. \n",
    "\n",
    "<img src=\"images/attmask.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datacamp Blog](https://www.datacamp.com/tutorial/how-transformers-work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae02759-90ea-4f0f-b17b-4a8651cb0d7c",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the masked multihead attention is added to the residual and then undergoes layer normalization for each token row to stabilise activations and gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50259ce3-fb70-47c1-9aea-83113a861b42",
   "metadata": {},
   "source": [
    "####  Cross Attention\n",
    "\n",
    "This multihead attention module functions exactly as the one in the encoder layer, with the only different being that the input for Q and K comes from the output of the encoder block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578dea1-2db6-4f32-a86f-24d59d287729",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the multihead attention is added to the residual and then undergoes layer normalization for each token row to stabilise activations and gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1c085-b893-453b-9746-f452bd03506d",
   "metadata": {},
   "source": [
    "#### Feedforward Network\n",
    "\n",
    "The output of the cross attention is passed through a FFN similar to the one found in the encoder block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74be3e3-312c-4c89-ac42-bdef2b57be3d",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "\n",
    "The output of the feedforward network once again undergoes addition to residual followed by normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460b782-fcc5-41fb-b634-c192d418167b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Layer\n",
    "\n",
    "The final linear layer of the model acts as a classifier which prompts a score over each token in the model vocabulary. \n",
    "\n",
    "It uses the output matrix of decoder ( $N_l \\times N_{dim}$) and multiplies it with a trained weight matrix ($N_{dim} \\times N_v$) to generate a matrix of dimensions $N_l \\times N_v$ where each row represents the logit scores of that token for all the $v$ vocabulary items in the model vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0ed11-ab01-4dfc-a4c0-c0d42829fc87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Softmax\n",
    "\n",
    "The output of the linear classifier is used to convert the scores from linear classifier into a probability distribution over the vocabulary. \n",
    "\n",
    "The softmax is a differentiable function and is used to calculate the cross entropy loss $\\sum_{i=1}^n x_i log(p_i)$ for each token which is propagated backward through the network during the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3d43e-7da6-4579-bed8-add47f32dacd",
   "metadata": {},
   "source": [
    "\n",
    "$$s(x_i)= \\frac{e^{x_i}}{\\sum_{j=1}^n e^j}$$\n",
    "\n",
    "\n",
    "<img src=\"images/llineartrans.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Datacamp Blog](https://www.datacamp.com/tutorial/how-transformers-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefd118-7dc3-48c5-a387-eb3a56f24c9b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training of a typical transformer involves the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656eaa0-760b-4da9-88c8-b19c76c823e7",
   "metadata": {},
   "source": [
    "\n",
    "1. **Forward Pass of Encoder**: The forward pass of encoder takes in the entire training sequence and generates an encoder output which serves as input to cross-attention of decoder layer.\n",
    "2. **Forward Pass of Decoder**: The forward pass of decoder takes in one token input at a time of the sequence and the encoder output, to generate an output matrix\n",
    "3. **Classification**: The decoder output is used to generate a score vector over the model voacbulary.\n",
    "4. **Softmax**: The softmaxed vector output is obtained for a sequence.\n",
    "5. **Ground Truth Encoding**: For each token in sequence, an equivalent one hot encoding is generated where the only 1 is the correct next token.\n",
    "\n",
    "<img src=\"images/onehot.png\" \n",
    "        alt=\"Picture\" \n",
    "        width=\"500\" \n",
    "        height=\"500\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Image Source: [Jay Alammar's Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "6. **Loss Calculation**: The loss between these two vectors is calculated in terms of encoding difficulty (Cross-Entropy Loss) or divergence (KL Divergence Loss) from one probability distribution to another. The cross entropy loss is most commonly used since the only difference from KL Divergence is the entropy of P which is constant.\n",
    "\n",
    "$$ H(P,Q)= -\\sum_{x=1}^j P(x_i)log_2(Q(x_i))$$\n",
    "\n",
    "7. **Backpropagation**: The gradients are calculated for all weights in the network and updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f8d4c-888f-4fb2-9d61-3d1b20a462b0",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "For the use of transformers in real world scenarios, the softmax vector is used to output a token from the vocabulary and this is known as decoding.\n",
    "\n",
    "There are various decoding methods for LLMs: \n",
    "\n",
    "1. **Greedy Decoding**: The output with highest probability from the softmax is chosen. \n",
    "2. **Beam Search**: Given a hyperparameter k, the algorithm tracks the top $k$ vocabulary tokens and chooses one that has the highest probability with all tokens so far. \n",
    "3. **Nucleus Sampling**: Given a hyperparameter p, the algorithm tracks the tokens with cumulative probability above $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792eb463-cbe3-4b68-bddb-ea3256fe70d6",
   "metadata": {},
   "source": [
    "## Features of Transformers\n",
    "\n",
    "There are two main architectural novelties in the transformer: \n",
    "\n",
    "1. **Parallelization**: Unlike RNNs, the tokens in a sequence do not have to be processed one by one and each token undergoes its own separate paralle flow through a transformer.\n",
    "2. **Attention Mechanism**: The parallelization in part is made possible by the attention mechanism which plays the role of hidden state of RNNs, allowing transformers to keep track of other sequence tokens (past or ahead) that the model processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4d6c4-9c2a-4076-9af7-ba37e5cb54dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Types of Transformers\n",
    "\n",
    "Since their introduction, several modifications have been made to adapt transformers for different tasks. The primary variants of this architecture include: \n",
    "\n",
    "- **Sequence-to-Sequence Transformers**: These models use the original transformer architecture with both encoder to decoder for sequence transduction tasks. Example: Bart, T5\n",
    "- **Autoencoder Transformers**: These models use only the encoder blocks of a transformer for understanding of a text via the representational capabilities of a model. Based on their training on text with masked words, they are also known as Masked Language Models (MLMs).\n",
    "- **Autoregressive Transformers**: These models use only the decoder blocks of a transformer for generating text. Based on their training with next token prediction, they are also known as Causal Language Models (CLMs). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1eb9b-f501-42c9-9ad6-13da6858afa5",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Transformers\n",
    "\n",
    "These models create contextualized embeddings (embeddings + positional encodings) for each word in a sequence and then pass them through encoder blocks (attention + MLP) followed by decoder blocks (masked attention + cross attention + MLP). \n",
    "\n",
    "- They are trained with sequence translation.\n",
    "- They are good at understanding text via their bidirectional attention mechanism and also generating text.\n",
    "- The output representations of encoder can be used for downstream tasks like token classification, sequence classification, feature analysis etc.\n",
    "- The output of decoder can be used for generating text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591931f-ec1b-41cb-b516-2fcb769eabc3",
   "metadata": {},
   "source": [
    "## Autoencoder Transformers\n",
    "\n",
    "The encoder models create contextualized embeddings (embeddings + positional encodings) for each word in a sequence and then pass them through encoder blocks (attention + MLP) \n",
    "\n",
    "- They are trained with masked language modelling.\n",
    "- They are good at understanding text via their bidirectional attention mechanism.\n",
    "- The output representations can be used for downstream tasks like token classification, sequence classification, feature analysis etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f918e-f59d-432a-90c2-e180b83d6c8b",
   "metadata": {},
   "source": [
    "## Autoregressive Transformers\n",
    "\n",
    "The encoder models create contextualized embeddings (embeddings + positional encodings) for each word in a sequence and then pass them through decoder blocks (masked attention + MLP) \n",
    "\n",
    "- They are trained with causal language modelling.\n",
    "- They are good at generating probability distributions over model vocabulary via their masked attention mechanism.\n",
    "- The output representations undergo decoding to generate tokens iteratively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a22b99-b251-4db2-99de-9dbc7f627e9b",
   "metadata": {},
   "source": [
    "# Implementing Transformers \n",
    "\n",
    "One of the most common ways to implement transformers is by building them with automated differentiation libraries like pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e204ae-532c-4ea9-ae13-dddf643090fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Output:\n",
      " [[-1.34163542 -0.44721181  0.44721181  1.34163542]\n",
      " [-1.34163542 -0.44721181  0.44721181  1.34163542]\n",
      " [-1.34163542 -0.44721181  0.44721181  1.34163542]]\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# layer norm \n",
    "import numpy as np \n",
    "class LayerNorm(): \n",
    "    def __init__(self,epsilon=1e-5):\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "    def forward (self, x): \n",
    "        mean=np.mean(x,axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        return x_normalized\n",
    "        \n",
    "# applying layer norm\n",
    "x = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])\n",
    "\n",
    "# Instantiate LayerNorm and normalize x\n",
    "layer_norm = LayerNorm()\n",
    "output = layer_norm.forward(x)\n",
    "\n",
    "print(\"Normalized Output:\\n\", output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cded45-d62c-4bbb-9688-ad354fec7590",
   "metadata": {},
   "source": [
    "# Huggingface Transformers\n",
    "\n",
    "Implementing and training transformers from scratch is not always feasible so we can use trained transformer models from Huggingface. \n",
    "\n",
    "The HuggingFace ecosystem is geared toward NLP and consists of 4 main libraries:\n",
    "\n",
    "- [Datasets](https://github.com/huggingface/datasets)\n",
    "- [Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "- [Transformers](https://github.com/huggingface/transformers)\n",
    "- [Accelerate](https://github.com/huggingface/accelerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e53f6-6d9d-4d31-8abe-ff0e2847f5a7",
   "metadata": {},
   "source": [
    " ## What is NLP? \n",
    "\n",
    " NLP is subfield of linguistics and machine learning which is focused on creating models that process, understand and generate natural language in a way that reflects human language abilities. \n",
    "\n",
    " Some common NLP tasks are: \n",
    "\n",
    " - Classification: It involves taking elements from a document and putting them into pre-specified categories. \n",
    " - Extraction: It involves isolating and returning specific parts from a document or set of document collection which are relevant to query\n",
    " - Summarization: It involves taking a document or set of documents and converting it into a shorter version without loss of information.\n",
    " - Generation: It involves creating more text related to a given input.\n",
    "\n",
    "While the most common work involves written text, NLP also finds applications in speech and vision related tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c9e00-6446-4025-a2a2-e45bf28ae7a5",
   "metadata": {},
   "source": [
    "## Transformers Library\n",
    "\n",
    "The HuggingFace Hub consists of hundreds of pretrained transformers which can be used with help of the \"transformers\" library that provides functionality for creating and using the models. The library uses a single API through which any model can be loaded, trained and saved. Some important features of the library are: \n",
    "\n",
    "- Each model is a simple PyTorch `nn.module` class or a TensorFlow `tf.keras.Model` class.\n",
    "- Each model in the hub has its own forward pass defined in a unique Config file, which does not include weights and is not shared across models.\n",
    "- The Config File (from `class transformers.PretrainedConfig`) can be edited separately for each model to allow us more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680200f-c00b-4a4f-bdad-1ed991c4842c",
   "metadata": {},
   "source": [
    "\n",
    "There are two main ways of using the models: \n",
    "\n",
    "- **Pipelines**: It is an end to end function that performs a pre specified NLP task on one or more texts. The pipeline tasks available are: `zero-shot-classification`, `sentiment-analysis`,`ner`,`summarization`,`text-generation`, `translation`,`question-answering`, `fill-mask`, `feature-extraction`, and many more. The total tasks stand at 17 currently. \n",
    "- **Checkpoints**: It is way of loading pre defined architecture and weights of a transformer and then defining the pre and post processing pipelines ourselves for better customisation.\n",
    "- **Architectures**: It is way of loading pre defined architecture of a transformer from model config and then loading weights separately before defining the pre and post processing pipelines ourselves for better customisation.\n",
    "\n",
    "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b43905-86e3-4689-81ea-7909dd47ee8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6e8f25-612a-41d1-a5fc-ec4eb496434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waiter', 'carpenter', 'lawyer', 'mechanic', 'doctor']\n",
      "['nurse', 'maid', 'waitress', 'cook', 'teacher']\n"
     ]
    }
   ],
   "source": [
    "# Pipelines with task\n",
    "# import pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipeline with task\n",
    "classifier= pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I'm doing really really good\")\n",
    "\n",
    "# pipeline with model\n",
    "pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n",
    "pipe(\"This restaurant is awesome\")\n",
    "\n",
    "# pipeline with task and model\n",
    "unmasker=pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "male_jobs= unmasker(\"The man works as a [MASK].\")\n",
    "print([dic['token_str'] for dic in male_jobs])\n",
    "female_jobs= unmasker(\"The woman works as a [MASK].\")\n",
    "print([dic['token_str'] for dic in female_jobs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439abd5-b559-437b-83ca-9a8652324d59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Checkpoints\n",
    "\n",
    "Once we go beyond pipelines and use checkpoints, we use the `AutoTokenizer and `AutoModel` classes which are wrappers over tokenizer and models in the library for easy initialization of tokenizer/model objects. If we use checkpoints, we need to do 3 important steps ourselves: \n",
    "\n",
    "- Preprocessing with `AutoTokenizer`\n",
    "- Inference with `AutoModel`\n",
    "- Postprocessing to get output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac7134-b69e-4cb5-af88-d542d48241d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Preprocessing \n",
    "\n",
    "The preprocessing for inputs is done as follows: \n",
    "\n",
    "- Import AutoTokenizer\n",
    "- Initialize AutoTokenizer object from a pretrained model checkpoint\n",
    "- Create input list\n",
    "- Call tokenizer object with inputs, padding, truncation and output tensor type. Padding indicates whether sequences of differing length should be padded to match max length and truncation indicates breaking up of inputs into multiple sequences if they are longer than allowed model sequence length.\n",
    "- The output is a dictionary with two keys- input_ids which is a tensor of dimension $seqnum \\times seqlen$ indicating token numbers and attention mask which is also a same dimension tensor of mask values for each token where 1 is non-mask and 0 is mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b012b661-f6d6-43d8-9440-406d738852b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9b059141914990bcbefca621cfe734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada1e265c51148d88be580ee82ada6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4c50e8e98a40eaa3f81caa18e1a4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293,  1996, 17174,   102],\n",
      "        [  101,  1045,  6283,  1996,  2208,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "from transformers import AutoTokenizer\n",
    "model_name= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name) # tokenizer configuration from specific model\n",
    "raw_inputs=[  # pass in sentence list\n",
    "    'I love the intro',\n",
    "    'I hated the game'\n",
    "]\n",
    "tokenized_inputs= tokenizer(raw_inputs, padding= True, truncation= True, return_tensors=\"pt\")\n",
    "print(tokenized_inputs) # outputs token id and mask tensors of input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13393f9-0a3f-4dde-bbfc-55f86974e2e5",
   "metadata": {},
   "source": [
    "#### Inference with AutoModel Only\n",
    "\n",
    "The model inference with tokenized inputs is done as follows: \n",
    "\n",
    "- Import AutoModel\n",
    "- Initialize AutoModel from a pretrained model checkpoint\n",
    "- Call model object with tokenized inputs\n",
    "- The output is a tensor of dimensions $seqnum \\times seqlen \\times model dimension$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bf3014-1a6b-475f-815c-ffe89011010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.5472,  0.0722,  0.0797,  ...,  0.5431,  1.1225, -0.3735],\n",
      "         [ 0.7301,  0.1646,  0.1477,  ...,  0.4137,  1.1830, -0.2681],\n",
      "         [ 0.9611,  0.3415,  0.3466,  ...,  0.3894,  1.0995, -0.3300],\n",
      "         [ 0.5608, -0.0158,  0.1010,  ...,  0.5690,  1.2197, -0.3903],\n",
      "         [ 0.4259,  0.2286,  0.2534,  ...,  0.5037,  0.9604, -0.3332],\n",
      "         [ 1.0663,  0.1271,  0.6909,  ...,  0.6291,  0.7586, -0.8868]],\n",
      "\n",
      "        [[-0.3951,  0.7698, -0.2996,  ...,  0.0231, -0.8558, -0.0593],\n",
      "         [-0.4719,  0.8989, -0.2922,  ..., -0.2249, -0.5333,  0.1423],\n",
      "         [-0.2880,  0.8173, -0.2044,  ..., -0.1231, -0.7169, -0.0454],\n",
      "         [-0.5774,  0.6085, -0.3825,  ..., -0.4900, -0.7831,  0.1544],\n",
      "         [-0.4643,  0.4916, -0.4042,  ..., -0.7778, -0.6882,  0.4220],\n",
      "         [ 0.0956,  0.5564, -0.3878,  ..., -0.2513, -0.6992, -0.1342]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "# inference with automodel\n",
    "from transformers import AutoModel\n",
    "model_name= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model=AutoModel.from_pretrained(model_name)\n",
    "outputs= model(**tokenized_inputs)\n",
    "print(outputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88583dc-2d9c-4e25-bbc7-0c3cc8e6c31c",
   "metadata": {},
   "source": [
    "#### Inference with AutoModel + Head\n",
    "\n",
    "The model output with head typically gives the loss and logit tensor of dimension $seqnum \\times classes$. The output can be used directly for analysis or it can be decoded to get natural language ouput. \n",
    "\n",
    "Instead of AutoModel, we can use AutoModel* where * can be anything from  `ForTokenClassification`, `ForSequenceClassification`,`ForQuestionAnswering`, `ForMaskedLM`, `ForCausalLM`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0f8c22-0ab8-4378-94e7-3c478c700c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2865,  4.6021],\n",
      "        [ 4.4335, -3.6342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# inference with automodel + head\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "outputs= model(**tokenized_inputs)\n",
    "print(outputs)\n",
    "print(outputs.logits.shape) # each row indicates a sequence and each column the logit score for a class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6e947-8c50-4cc3-8900-fc61db5d16a1",
   "metadata": {},
   "source": [
    "#### Postprocessing\n",
    "\n",
    "The logit outputs need to be converted into probability scores via softmax to be turned into predictions. The predictions can be further decoded into a class or a token as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eadd097d-c004-46a9-a49e-ccd69d178b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3793e-04, 9.9986e-01],\n",
      "        [9.9969e-01, 3.1341e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "# postprocessing\n",
    "from torch import nn as nn\n",
    "predictions= nn.functional.softmax(outputs.logits, dim=-1) # softmax across last dimension\n",
    "print(predictions)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49df51a-c400-4b93-bb3b-fbf571b4b9c6",
   "metadata": {},
   "source": [
    "### Using Architectures\n",
    "\n",
    "Using architectures gives us the most control over the model. Unlike using checkpoints, here we use the `Model` and `Config` classes to load architectures (i.e defined forward pass) and then we initialize weights (via training or loading) before we go onto initializing a tokenizer/model object. \n",
    "\n",
    "The steps typically include: \n",
    "\n",
    "- The `Config` file gives two important things-  configuration class to be used for model and the model class to be used.\n",
    "- The model configuration is instantiated and is equivalent to a model blueprint.\n",
    "- The model config and the model class are combined to initialize a model.\n",
    "- Now, the weights are loaded via a `Model` file to properly initialize a pretrained tokenizer and model.\n",
    "- The next steps are similar to using checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0007d-5fe4-497b-964c-9d412053f999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
