{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d637067-f03d-4026-8eb9-6b4c1b32f624",
   "metadata": {},
   "source": [
    "# What is reinforcement learning? \n",
    "\n",
    "In very simple terms, an agent learns to pick actions in an environment to get more reward over time. The point is to make the agent learn the process of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b040fca-d384-4c5e-b1fc-e0f0e95e4348",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "Also known as MDPs, they provide a way to formalize sequential decision making. In an MDP, we have: \n",
    "\n",
    "1. Environment: A surrounding or scenario which defines our world.\n",
    "2. Agent: A model that interacts with the environment it is placed in.\n",
    "3. State: A configuration of the environment that the agent faces at a given time step.\n",
    "4. Action: A step taken by the agent to bring about a change of state in its environment.\n",
    "5. Reward: A score given to the agent as a result of that action.\n",
    "\n",
    "The whole process repeats over and over, referred to as a trajectory, where the agent's aim is to  maximize cumulative reward i.e the reward it gets across time steps (not just at one time step). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8162f05-f4a4-4ba7-8331-ffba6a148aa1",
   "metadata": {},
   "source": [
    "### Formal Problem Notation\n",
    "\n",
    "In an MDP, we have set of states **S** , a set of actions **A**, and a set of rewards **R**. Each of them has a finite set of elements. \n",
    "\n",
    "1. At each time step *t*= **0,1,2,...**, agent is at a state $S_t \\in S$ and takes an action $A_t \\in A$. So, for each time step, we have a state-action pair $(S_t, A_t)$.\n",
    "2. At $S_{t+1}$, we get numerical reward $R_{t+1} \\in R$.\n",
    "3. The reward can be expressed as a function of state and time.\n",
    "   $$f(S_t,A_t)= R_{t+1}$$\n",
    "4. Th trajectory is a sequence of $S, A, R$.\n",
    "   $$ S_0, A_0, R_1, S_1, A_1, R_2, ...$$\n",
    "5. Given **S** and **R** are finite, the random variables $S_t$ and $R_t$ have well defined probability distributions. The transition probability of going from action $a$ in state $s$ to state $s'$ with reward $r$ can be defined.\n",
    "   $$ p(s',r|s,a)= Probability(S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145a250-cd9a-42cc-891e-623510c41f28",
   "metadata": {},
   "source": [
    "### Expected Return\n",
    "\n",
    "In an MDP, the main driving force behind the agent is maximizing it's cumulative reward, which we also know as *Expected Return*. We can define it as \n",
    "\n",
    " $$ G_t= R_{t+1}+ R_{t+2}+ R_{t+3}+ ...+ R_T $$ \n",
    "\n",
    "where *T* is the total number of time step and the goal of the agent is to maximize $G$.\n",
    "\n",
    "However, there is an issue if we just add up rewards forever:\n",
    "\n",
    "1. **Infinity problem**: If episodes donâ€™t end , the sum may diverge (e.g., robot that keeps getting +1 reward each step = infinite return). \n",
    "\n",
    "2. **Unrealistic planning**: We care more about sooner rewards than way in the future. For example, a robot that delivers coffee in 10 steps is better than one that delivers in 1000 steps, even if both eventually succeed.\n",
    "\n",
    "To combat these issues, we introduce the discounted expected return. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593204ed-7efb-4f4a-b7b2-719001453c5f",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "We define the discount rate to be a number $\\gamma$ between 0 and 1 such that each term in the expected return function will be multiplied by $\\gamma$: \n",
    "\n",
    "$$G_t= \\gamma R_{t+1}+ \\gamma^2R_{t+2}+ \\gamma^3R_{t+3}+ ...$$\n",
    "$$G_t= \\sum_{k=0}^\\inf \\gamma^kR_{t+k+1}$$\n",
    "\n",
    "One key feature of discounted return is that agent values immediate rewards more due to lower weights of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bae4a-6ca9-443f-a2bc-23ad4f876d33",
   "metadata": {},
   "source": [
    "## Policy and Value Functions\n",
    "\n",
    "When it comes to agents selecting an action at a given state, there are two factors of consideration: Policies and Value Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c06808-e558-4eb7-b7e4-d8ae6924d6e0",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "A policy is the function that describes the probability of picking different actions from each possible state. For each state $s\\in S$, the policy $\\pi$ is a probability distribution over all actions $a\\in A(s)$ .\n",
    "\n",
    "At any time $t$, under a policy $\\pi$, the probability of taking action $a$ in state $s$ is given by $\\pi(a|s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4ea55-a352-452c-b84c-d430a7829c3f",
   "metadata": {},
   "source": [
    "### Value Functions\n",
    "\n",
    "A value function is the function over states or state-action pairs that estimates two possible things: \n",
    "\n",
    "1. State-Value Function: How good it is for the agent to be in a given state. \n",
    "2. Action-Value Function: How good it is for the agent to pick an action in a given state.\n",
    "\n",
    "Value functions are thus defined in terms of expected returns on acting. The expected returns on acting is determined in terms the policy. Thus value functions are defined in terms of the policy. \n",
    "\n",
    "**State Value Function** for a state $s$ is given in terms of expected returns on starting from $s$ at time $t$ and following policy $\\pi$ thereafter: \n",
    "$$v_{\\pi}(s)= E(G_t|S_t=s)$$\n",
    "\n",
    "**Action Value Function/Q Function** for an action $a$ in state $s$ is given in terms of expected returns on starting from $s$ at time $t$, taking action $a$,  and following policy $\\pi$ thereafter: \n",
    "$$q_{\\pi}(s,a)= E(G_t|S_t=s,A_t=a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4e4f8-4085-4f50-97b8-063bd4f02e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optimization\n",
    "\n",
    "Any learning algorithm learns by optimizing a function. In traditional ML, the cost function is optimized. In RL, what do we optimize? We optimize the policy and the value functions!\n",
    "\n",
    "**Optimal Policy**: A policy is considered optimal if the expected return of $\\pi$ is greater than or equal to the expected returns of all other policies $\\pi'$. \n",
    "\n",
    "$$\\pi>\\pi' \\iff v_{\\pi}(s)> v_{\\pi'}(s), \\forall s\\in S$$\n",
    "\n",
    "**Optimal State-Value Function**: The optimal policy has an associated optimal state-value function $v*$ that gives the largest return possible by any policy $\\pi$ for each state. \n",
    "$$v*(s)=max_{\\pi}v_{\\pi}(s),  \\forall s \\in S$$\n",
    "\n",
    "**Optimal Action-Value/Q Function**: The optimal policy has an associated optimal action-value function $q*$ that gives the largest return possible by any policy $\\pi$ for each state-action pair. \n",
    "$$q*(s,a)=max_{\\pi}q_{\\pi}(s,a),  \\forall s \\in S , \\forall a \\in A$$\n",
    "\n",
    "The $q*$ function must satisfy the Bellman Optimality which states that for any state-action pair at time $t$, the expected return of starting in state $s$ and selecting action $a$ and following optimal policy $\\pi$ thereafter i.e *q-value* of $(s,a)$ is the expected reward plus the maximum discounted return that can be achieved from any possible next $(s',a')$\n",
    "\n",
    "$$ q*(s,a)= E[R_{t+1} + \\gamma max_{a'}q*(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9cbf6-74e2-4eb5-a06e-da14a28d08bc",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "It is the process by which reinforcement learning occurs. \n",
    "\n",
    "* Objective: Learn the optimal q-values for each (s,a) pair and thus find the optimal policy. \n",
    "* Value Iteration: The algorithm iteratively updates the Q-values for each state-action pair until it converges to $q*$. \n",
    "* Episodes: The number of episodes i.e number of times agent terminates is set beforehand. Within each episode, we have multiple time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd79efd-4b93-46d0-b3d4-c178e5a8ddfd",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "The Bellman Equation gives us the q-value at optimality\n",
    "\n",
    "$$ q*(s,a)= E[R_{t+1} + \\gamma max_{a'}q*(s',a')]$$\n",
    "\n",
    "Now, in our learning process , we will get some q values. We calculate error $\\delta$\n",
    "\n",
    "$$ \\delta= q*(s,a) - q(s,a)$$\n",
    "\n",
    "Putting in the value of optimal q from Bellman Equation, we get error $\\delta$\n",
    "\n",
    "$$ \\delta= [R_{t+1} + \\gamma max_{a'}q*(s',a')] - q(s,a)$$\n",
    "\n",
    "To get the new q-value, we add this $\\delta$ to the old q value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ffe1b-3c1a-47fb-99d1-925364c2b33c",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "\n",
    "The training process is as follows:\n",
    "\n",
    "1. We initialize with an empty q-value matrix $X_{m\\times n}$ where $m$= number of states and $n$= number of actions.\n",
    "2. At each action of agent, the new q-value is calculated with learning rate $\\alpha$\n",
    "$$ q_{new}(s,a)= (1-\\alpha)q_{old}(s,a) + \\alpha (R_{t+1}+ \\gamma max q(s',a'))$$\n",
    "\n",
    "3. The agent can use the q-values to go down paths that maximize q-values at every state.\n",
    "4. This whole process is repeated until a termination condition is met or fixed number of steps have been taken.\n",
    "5. The optimal q-function gives us the ultimate optimized policy.\n",
    "\n",
    "The exploration-exploitation tradeoff determines how much agent tries new paths in the environment vs how much it tries to maximize rewards based on what it has already seen. It is typically done through $\\epsilon$-greedy strategy. \n",
    "\n",
    "1. We initially set $\\epsilon=1$ and decay it in each step.\n",
    "2. At every time step, we choose a random number $r$ where we explore if $r>\\epsilon$ and exploit if $r<\\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd0900-d420-4370-8d48-b73d42e3e088",
   "metadata": {},
   "source": [
    "# Implementing RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca77e544-8196-4d71-9eff-49f14b9247ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen Lake Game\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random \n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d447125-c30f-4ba1-b5a3-7b484b6ac1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing environment and agent\n",
    "env = gym.make('FrozenLake-v1', render_mode='ansi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
